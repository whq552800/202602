{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a8aac89-adbd-4ce0-b295-4c0a83dca0f0",
   "metadata": {},
   "source": [
    "## Systematic ablation analysis of modelling operations based on out-of-sample prediction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68199be3-72e3-499e-be2b-0e46cd5dbfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "plot for CV_RMSE across model permutations (by age)\n",
    "============================================================================\n",
    "Input:\n",
    "  For each age folder:\n",
    "    FULL_EXHAUST_4EXPO_results_all_models.csv\n",
    "Output:\n",
    "  AGE_<age>/ED_FIG_CV_ops_<age>.png/.pdf\n",
    "\n",
    "Notes:\n",
    "- Pink: models INCLUDING the term/operation\n",
    "- Grey: models EXCLUDING the term/operation\n",
    "- Black bar: mean(without) - mean(with)  (positive => including improves RMSE)\n",
    "- Red star: Welch t-test p < 0.05\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import gaussian_kde, ttest_ind\n",
    "\n",
    "# =========================\n",
    "# STYLE\n",
    "# =========================\n",
    "mpl.rcParams[\"pdf.fonttype\"] = 42\n",
    "mpl.rcParams[\"ps.fonttype\"]  = 42\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"Arial\", \"SimHei\", \"DejaVu Sans\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "# =========================\n",
    "# PATHS\n",
    "# =========================\n",
    "ROOT = r\"D:\\AAUDE\\paper_v2\\paper2\\data\\model_outputs\\FULL_EXHAUST_4EXPO_BY_AGE\"\n",
    "AGE_DIRS = {\n",
    "    \"total\": os.path.join(ROOT, \"AGE_total\"),\n",
    "    \"u5\":    os.path.join(ROOT, \"AGE_u5\"),\n",
    "    \"5_65\":  os.path.join(ROOT, \"AGE_5_65\"),\n",
    "    \"65p\":   os.path.join(ROOT, \"AGE_65p\"),\n",
    "}\n",
    "\n",
    "CSV_NAME = \"FULL_EXHAUST_4EXPO_results_all_models.csv\"\n",
    "RMSE_COL = \"CV_RMSE\"\n",
    "\n",
    "# KDE \n",
    "KDE_N = 400\n",
    "\n",
    "\n",
    "MIN_N = 25\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def _as_set(x):\n",
    "    if pd.isna(x):\n",
    "        return set()\n",
    "    s = str(x).strip()\n",
    "    if s == \"\" or s.upper() == \"NO_INTER\":\n",
    "        return set()\n",
    "    # ‰Ω†ÁöÑ interactions Â≠óÊÆµÊòØ \"HAP_X+CLIM1_X\" ËøôÁßç\n",
    "    return set([t.strip() for t in s.split(\"+\") if t.strip()])\n",
    "\n",
    "def kde_curve(values, xgrid):\n",
    "    values = np.asarray(values, dtype=float)\n",
    "    values = values[np.isfinite(values)]\n",
    "    if values.size < 5:\n",
    "        return np.zeros_like(xgrid)\n",
    "    # Èò≤Ê≠¢ÂÖ®Áõ∏ÂêåÂØºËá¥ KDE Â¥©Ê∫É\n",
    "    if np.nanstd(values) < 1e-12:\n",
    "        y = np.zeros_like(xgrid)\n",
    "        # Âú®ËØ•ÁÇπÈôÑËøëÂÅö‰∏Ä‰∏™Á™ÑÁöÑ‚ÄúËÑâÂÜ≤‚Äù\n",
    "        j = np.argmin(np.abs(xgrid - float(values[0])))\n",
    "        y[max(0, j-1):min(len(y), j+2)] = 1.0\n",
    "        return y\n",
    "    kde = gaussian_kde(values)\n",
    "    y = kde(xgrid)\n",
    "    return y\n",
    "\n",
    "def welch_p(a, b):\n",
    "    a = np.asarray(a, dtype=float); b = np.asarray(b, dtype=float)\n",
    "    a = a[np.isfinite(a)]; b = b[np.isfinite(b)]\n",
    "    if a.size < MIN_N or b.size < MIN_N:\n",
    "        return np.nan\n",
    "    try:\n",
    "        return float(ttest_ind(a, b, equal_var=False).pvalue)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def mean_diff(with_vals, without_vals):\n",
    "    with_vals = np.asarray(with_vals, dtype=float)\n",
    "    without_vals = np.asarray(without_vals, dtype=float)\n",
    "    with_vals = with_vals[np.isfinite(with_vals)]\n",
    "    without_vals = without_vals[np.isfinite(without_vals)]\n",
    "    if with_vals.size < 5 or without_vals.size < 5:\n",
    "        return np.nan\n",
    "    return float(np.nanmean(without_vals) - np.nanmean(with_vals))\n",
    "\n",
    "# =========================\n",
    "# TERM DEFINITIONS (operations)\n",
    "# =========================\n",
    "def build_terms(df):\n",
    "    \"\"\"\n",
    "    ËøîÂõû list[ (label, mask_func) ]\n",
    "    mask_func(df) -> boolean array, True Ë°®Á§∫‚ÄúÂåÖÂê´ËØ•Êìç‰Ωú‚Äù\n",
    "    \"\"\"\n",
    "    # ÂÖºÂÆπÂàóÂêçÂèØËÉΩÁº∫Â§±Ôºömask_func ÂÜÖÈÉ®Ë¶ÅÂÆâÂÖ®\n",
    "    def col_eq(col, val):\n",
    "        if col not in df.columns:\n",
    "            return np.zeros(len(df), dtype=bool)\n",
    "        return (df[col].astype(str) == str(val)).to_numpy()\n",
    "\n",
    "    def has_inter(term):\n",
    "        if \"interactions\" not in df.columns:\n",
    "            return np.zeros(len(df), dtype=bool)\n",
    "        return df[\"interactions\"].apply(lambda x: term in _as_set(x)).to_numpy()\n",
    "\n",
    "    terms = [\n",
    "        (\"HAP log1p\",          lambda d: col_eq(\"hap\", \"hap_log1p\")),\n",
    "        (\"PM2.5 log1p\",        lambda d: col_eq(\"pm25\", \"pm25_log1p\")),\n",
    "        (\"Z-score all vars\",   lambda d: col_eq(\"zall\", \"zall\")),\n",
    "        (\"Climate: AH resid | TAVG\", lambda d: col_eq(\"clim_struct\", \"both_AHresid_on_TAVG\")),\n",
    "        (\"Climate: TAVG resid | AH\", lambda d: col_eq(\"clim_struct\", \"both_TAVGresid_on_AH\")),\n",
    "        (\"SDI scheme: Q4 merge top\", lambda d: col_eq(\"sdi_scheme\", \"Q4_mergeTop\")),\n",
    "        (\"Coding: star_expand\",       lambda d: col_eq(\"coding\", \"star_expand\")),\n",
    "        (\"Center interaction within SDI group\", lambda d: col_eq(\"center_inter\", \"centerWithinQ\")),\n",
    "        # interactions presence\n",
    "        (\"Interaction includes HAP\",   lambda d: has_inter(\"HAP_X\")),\n",
    "        (\"Interaction includes PM2.5\", lambda d: has_inter(\"PM25_X\")),\n",
    "        (\"Interaction includes CLIM1\", lambda d: has_inter(\"CLIM1_X\")),\n",
    "        (\"Interaction includes CLIM2\", lambda d: has_inter(\"CLIM2_X\")),\n",
    "    ]\n",
    "    return terms\n",
    "\n",
    "# =========================\n",
    "# PLOT ONE AGE\n",
    "# =========================\n",
    "def plot_one_age(age, csv_fp, out_dir):\n",
    "    df = pd.read_csv(csv_fp)\n",
    "\n",
    "    if RMSE_COL not in df.columns:\n",
    "        raise ValueError(f\"Missing {RMSE_COL} in {csv_fp}\")\n",
    "\n",
    "    df[RMSE_COL] = pd.to_numeric(df[RMSE_COL], errors=\"coerce\")\n",
    "    df = df[np.isfinite(df[RMSE_COL])].copy()\n",
    "\n",
    "    if len(df) == 0:\n",
    "        raise RuntimeError(f\"No finite RMSE rows for age={age}\")\n",
    "\n",
    "    # x range for plots\n",
    "    x_min = 0.2\n",
    "    x_max = 1\n",
    "    if not np.isfinite(x_min) or not np.isfinite(x_max) or x_max <= x_min:\n",
    "        x_min = float(np.nanmin(df[RMSE_COL]))\n",
    "        x_max = float(np.nanmax(df[RMSE_COL]))\n",
    "    pad = 0.05 * (x_max - x_min + 1e-12)\n",
    "    x_min -= pad; x_max += pad\n",
    "    xgrid = np.linspace(x_min, x_max, KDE_N)\n",
    "\n",
    "    terms = build_terms(df)\n",
    "\n",
    "    # rows: 1 unconditional + n_terms\n",
    "    n_rows = 1 + len(terms)\n",
    "\n",
    "    fig_h = 6 * n_rows  # Ëá™ÈÄÇÂ∫îÈ´òÂ∫¶\n",
    "    fig_w = 8\n",
    "    fig, axes = plt.subplots(n_rows, 1, figsize=(fig_w, fig_h), sharex=True)\n",
    "\n",
    "    # ----- Row 0: unconditional density\n",
    "    ax0 = axes[0]\n",
    "    y0 = kde_curve(df[RMSE_COL].to_numpy(), xgrid)\n",
    "    ax0.fill_between(xgrid, 0, y0, alpha=0.25)\n",
    "    ax0.plot(xgrid, y0, lw=1.5)\n",
    "    ax0.set_ylabel(\"Density\")\n",
    "    ax0.set_title(f\"{age}: Unconditional OOS RMSE density (all models)\", fontsize=13)\n",
    "    COLOR_WITH    = \"#E64B35\"   # ÊüîÂíåÁ∫¢ / Á≤â\n",
    "    COLOR_WITHOUT = \"#4D4D4D\"   # Ê∑±ÁÅ∞\n",
    "\n",
    "    # ----- Other rows: conditional densities\n",
    "    for i, (label, mask_func) in enumerate(terms, start=1):\n",
    "        ax = axes[i]\n",
    "        m = mask_func(df)\n",
    "\n",
    "        with_vals = df.loc[m, RMSE_COL].to_numpy()\n",
    "        wo_vals   = df.loc[~m, RMSE_COL].to_numpy()\n",
    "\n",
    "        y_with = kde_curve(with_vals, xgrid)\n",
    "        y_wo   = kde_curve(wo_vals, xgrid)\n",
    "\n",
    "        # grey = without\n",
    "        ax.fill_between(xgrid, 0, y_wo, color=COLOR_WITHOUT, alpha=0.30)\n",
    "        ax.plot(xgrid, y_wo, color=COLOR_WITHOUT, lw=1.0)\n",
    "        \n",
    "        # pink = with\n",
    "        ax.fill_between(xgrid, 0, y_with, color=COLOR_WITH, alpha=0.30)\n",
    "        ax.plot(xgrid, y_with, color=COLOR_WITH, lw=1.0)\n",
    "\n",
    "\n",
    "        # labels & small stats\n",
    "        n_with = int(np.isfinite(with_vals).sum())\n",
    "        n_wo   = int(np.isfinite(wo_vals).sum())\n",
    "\n",
    "        dmean = mean_diff(with_vals, wo_vals)  # mean(without) - mean(with)\n",
    "        pval = welch_p(with_vals, wo_vals)\n",
    "\n",
    "        # Âú®Ë°åÂÜÖÂ∑¶‰æßÂÜô‰ø°ÊÅØ\n",
    "        txt = f\"{label}  |  with={n_with}  without={n_wo}\"\n",
    "        ax.text(0.01, 0.78, txt, transform=ax.transAxes, fontsize=10)\n",
    "\n",
    "        # ÈªëÁ∫øÔºöÂπ≥ÂùáÂ∑ÆÔºàÁî® x ËΩ¥‰ΩçÁΩÆË°®ËææÔºâ\n",
    "        # ÁîªÂú®ÂØÜÂ∫¶ÂõæÂ∫ïÈÉ®ÈôÑËøëÔºö‰ªé mean_with Âà∞ mean_wo\n",
    "        if np.isfinite(dmean) and n_with >= 5 and n_wo >= 5:\n",
    "            mu_with = float(np.nanmean(with_vals))\n",
    "            mu_wo   = float(np.nanmean(wo_vals))\n",
    "            ybar = 0.02 * max(np.nanmax(y_with), np.nanmax(y_wo), 1e-6)\n",
    "            # ax.plot([mu_with, mu_wo], [ybar, ybar], lw=2.2, color=\"k\")\n",
    "\n",
    "            # Á∫¢ÊòüÔºöÊòæËëóÊÄß\n",
    "            # if np.isfinite(pval) and pval < 0.05:\n",
    "            #     xm = 0.5 * (mu_with + mu_wo)\n",
    "            #     ax.text(xm, ybar * 1.6, \"‚òÖ\", color=\"red\", ha=\"center\", va=\"bottom\", fontsize=14)\n",
    "\n",
    "            # Âú®Âè≥‰æßÊ†áÊ≥®Â∑ÆÂÄº\n",
    "            ax.text(0.99, 0.78,\n",
    "                    f\"Œîmean(without-with)={dmean:+.3f} | p={pval:.3g}\" if np.isfinite(pval) else f\"Œîmean={dmean:+.3f} | p=NA\",\n",
    "                    transform=ax.transAxes, ha=\"right\", fontsize=10)\n",
    "\n",
    "        ax.set_yticks([])\n",
    "        ax.set_ylabel(\"\")\n",
    "\n",
    "    axes[-1].set_xlabel(\"Out-of-sample RMSE (CV_RMSE)\")\n",
    "\n",
    "    # legend (ÁÆÄÂåñ)\n",
    "    # Áî®‰∏§Êù°ËôöÊãüpatchËØ¥ÊòéÈ¢úËâ≤Âê´‰πâ\n",
    "    from matplotlib.patches import Patch\n",
    "    handles = [\n",
    "        Patch(facecolor=COLOR_WITHOUT, alpha=0.30, label=\"without term\"),\n",
    "        Patch(facecolor=COLOR_WITH,    alpha=0.30, label=\"with term\"),\n",
    "    ]\n",
    "\n",
    "    # ÊîæÂú®È°∂ÈÉ®Âè≥‰æß\n",
    "    axes[0].legend(handles=handles, loc=\"upper right\", frameon=False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    out_png = os.path.join(out_dir, f\"ED_FIG_CV_ops_{age}.png\")\n",
    "    out_pdf = os.path.join(out_dir, f\"ED_FIG_CV_ops_{age}.pdf\")\n",
    "    plt.savefig(out_png, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.savefig(out_pdf, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"[Saved] {out_png}\")\n",
    "    print(f\"[Saved] {out_pdf}\")\n",
    "\n",
    "# =========================\n",
    "# RUN ALL AGES\n",
    "# =========================\n",
    "def main():\n",
    "    for age, d in AGE_DIRS.items():\n",
    "        csv_fp = os.path.join(d, CSV_NAME)\n",
    "        if not os.path.exists(csv_fp):\n",
    "            print(\"[Skip] missing:\", csv_fp)\n",
    "            continue\n",
    "        print(\"\\n\" + \"=\"*90)\n",
    "        print(\"AGE:\", age)\n",
    "        print(\"CSV:\", csv_fp)\n",
    "        print(\"=\"*90)\n",
    "        plot_one_age(age, csv_fp, d)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fcfe6b-62b7-49a3-98f6-beadeb3ccd01",
   "metadata": {},
   "source": [
    "## Theory-constrained exhaustive structural model search with stability filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14893efb-a7dc-4c8a-b986-07e66c097088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This procedure does not aim to identify the statistically best-fitting model,\n",
    "but rather to select a stable and interpretable structural representation consistent with prior theoretical considerations.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# STYLE\n",
    "# =========================\n",
    "mpl.rcParams[\"pdf.fonttype\"] = 42\n",
    "mpl.rcParams[\"ps.fonttype\"]  = 42\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"Arial\", \"SimHei\", \"DejaVu Sans\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "# =========================\n",
    "# PATHS\n",
    "# =========================\n",
    "IN_FP = r\"D:\\AAUDE\\paper_v2\\paper2\\data\\model_outputs\\panel0_1990_2019_direct_meteo_GBDPM_HAP_lui.csv\"\n",
    "OUT_ROOT = r\"D:\\AAUDE\\paper_v2\\paper2\\data\\model_outputs\\FULL_EXHAUST_4EXPO_BY_AGE\"\n",
    "os.makedirs(OUT_ROOT, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "Y0, Y1 = 1990, 2019\n",
    "CV_K   = 5\n",
    "SEED   = 123\n",
    "SE_TYPE = \"HC1\"\n",
    "VIF_CAP = 20.0  # Á®≥ÂÆöÊÄßÈòàÂÄºÔºöÂª∫ËÆÆ20ÔºõÂ§™‰∏•Ê†ºÂèØË∞É30\n",
    "\n",
    "# ‚úÖÊñ∞Â¢ûÔºöÂõõ‰∏™‰∏ªÊïàÂ∫îÈÉΩÂøÖÈ°ªÊòæËëó\n",
    "P_MAIN_CAP = 0.05  # ÂèØË∞ÉÔºö0.05/0.10\n",
    "\n",
    "AGE_SPECS = {\n",
    "    \"total\": {\"y\": \"uri_total\", \"pop\": \"pop_total\"},\n",
    "    \"u5\":    {\"y\": \"uri_u5\",    \"pop\": \"pop_u5\"},\n",
    "    \"5_65\":  {\"y\": \"uri_5_65\",  \"pop\": \"pop_5_65\"},\n",
    "    \"65p\":   {\"y\": \"uri_65p\",   \"pop\": \"pop_65p\"},\n",
    "}\n",
    "\n",
    "# Êö¥Èú≤ÂàóÂêçÔºàÈù¢ÊùøÊñá‰ª∂‰∏≠Ôºâ\n",
    "HAP_COL  = \"hap_pm_pw\"\n",
    "PM25_COL = \"pm25_pw\"\n",
    "TAVG_COL = \"tavg_pw_C\"\n",
    "AH_COL   = \"ah_pw\"\n",
    "SDI_COL  = \"sdi\"\n",
    "DENS_COL = \"density_total_pkm2\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def require_cols(df, cols):\n",
    "    miss = [c for c in cols if c not in df.columns]\n",
    "    if miss:\n",
    "        raise ValueError(f\"Missing columns: {miss}\")\n",
    "\n",
    "def safe_log(x, floor=1e-12):\n",
    "    return np.log(np.clip(x, floor, None))\n",
    "\n",
    "def safe_log1p(x):\n",
    "    return np.log1p(np.clip(x, 0.0, None))\n",
    "\n",
    "def zscore(s):\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    mu = np.nanmean(s)\n",
    "    sd = np.nanstd(s)\n",
    "    if not np.isfinite(sd) or sd <= 1e-12:\n",
    "        sd = 1.0\n",
    "    return (s - mu) / sd\n",
    "\n",
    "def kfold_pos_indices(n, k=5, seed=123):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = np.arange(n)\n",
    "    rng.shuffle(idx)\n",
    "    return np.array_split(idx, k)\n",
    "\n",
    "def compute_vif_exog(exog, names):\n",
    "    X = pd.DataFrame(exog, columns=names)\n",
    "    if \"Intercept\" in X.columns:\n",
    "        X = X.drop(columns=[\"Intercept\"])\n",
    "    keep = [c for c in X.columns if np.nanstd(X[c].to_numpy()) > 1e-12]\n",
    "    X = X[keep].copy()\n",
    "    if X.shape[1] <= 1:\n",
    "        return pd.DataFrame({\"term\": X.columns, \"VIF\": np.nan})\n",
    "    X = X.fillna(X.mean(numeric_only=True))\n",
    "    vifs = []\n",
    "    for i, c in enumerate(X.columns):\n",
    "        try:\n",
    "            v = float(variance_inflation_factor(X.values, i))\n",
    "        except Exception:\n",
    "            v = np.nan\n",
    "        vifs.append((c, v))\n",
    "    return pd.DataFrame(vifs, columns=[\"term\",\"VIF\"]).sort_values(\"VIF\", ascending=False)\n",
    "\n",
    "def make_bins_train_only(train_sdi, q=5):\n",
    "    train_sdi = np.asarray(train_sdi, dtype=float)\n",
    "    train_sdi = train_sdi[np.isfinite(train_sdi)]\n",
    "    if train_sdi.size < max(20, q*4):\n",
    "        return None\n",
    "    qs = np.nanquantile(train_sdi, np.linspace(0, 1, q+1))\n",
    "    qs2 = qs.copy()\n",
    "    for i in range(1, len(qs2)):\n",
    "        if qs2[i] <= qs2[i-1]:\n",
    "            qs2[i] = qs2[i-1] + 1e-12\n",
    "    qs2[0]  = -np.inf\n",
    "    qs2[-1] = np.inf\n",
    "    return qs2\n",
    "\n",
    "def assign_q_from_bins(sdi, bins, labels):\n",
    "    cat = pd.cut(sdi, bins=bins, labels=labels, include_lowest=True)\n",
    "    return cat.astype(\"category\")\n",
    "\n",
    "def merge_top(cat):\n",
    "    s = cat.astype(str).replace({\"Q4\":\"TOP\",\"Q5\":\"TOP\"})\n",
    "    return pd.Categorical(s, categories=[\"Q1\",\"Q2\",\"Q3\",\"TOP\"], ordered=True)\n",
    "\n",
    "def add_sdi_q_foldwise(d_raw, q=5, mergeTop=False, train_pos=None):\n",
    "    \"\"\"\n",
    "    foldÂÜÖÊåâ train_pos ËÆ°ÁÆóÂàÜ‰ΩçÈòàÂÄºÔºåÊò†Â∞ÑÂà∞ÂÖ®‰ΩìÔºõÈÅøÂÖçCVÊ≥ÑÊºè„ÄÇ\n",
    "    d_raw ÈúÄÂåÖÂê´ rid=0..n-1\n",
    "    \"\"\"\n",
    "    dd = d_raw.copy()\n",
    "    labels = [f\"Q{i}\" for i in range(1, q+1)]\n",
    "    if train_pos is None:\n",
    "        bins = make_bins_train_only(dd[\"sdi_mean\"].to_numpy(), q=q)\n",
    "        if bins is None:\n",
    "            dd[\"SDI_Q\"] = pd.qcut(dd[\"sdi_mean\"], q=q, labels=labels).astype(\"category\")\n",
    "        else:\n",
    "            dd[\"SDI_Q\"] = assign_q_from_bins(dd[\"sdi_mean\"], bins, labels)\n",
    "    else:\n",
    "        train_sdi = dd.iloc[train_pos][\"sdi_mean\"].to_numpy()\n",
    "        bins = make_bins_train_only(train_sdi, q=q)\n",
    "        if bins is None:\n",
    "            bins = make_bins_train_only(dd[\"sdi_mean\"].to_numpy(), q=q)\n",
    "        dd[\"SDI_Q\"] = assign_q_from_bins(dd[\"sdi_mean\"], bins, labels)\n",
    "\n",
    "    dd = dd.dropna(subset=[\"SDI_Q\"]).copy()\n",
    "    if mergeTop:\n",
    "        dd[\"SDI_Q\"] = merge_top(dd[\"SDI_Q\"])\n",
    "        dd = dd.dropna(subset=[\"SDI_Q\"]).copy()\n",
    "    return dd\n",
    "\n",
    "def add_center_within_q(dd, exposures, center_inter):\n",
    "    d = dd.copy()\n",
    "    if not center_inter:\n",
    "        return d\n",
    "    for x in exposures:\n",
    "        d[f\"{x}_CQ\"] = d[x] - d.groupby(\"SDI_Q\")[x].transform(\"mean\")\n",
    "    return d\n",
    "\n",
    "def build_formula(exposures, controls, inter_set, modifier_only=True, center_inter=False):\n",
    "    rhs = []\n",
    "    for x in exposures:\n",
    "        if x in inter_set:\n",
    "            x_int = f\"{x}_CQ\" if center_inter else x\n",
    "            if modifier_only:\n",
    "                rhs.append(f\"{x} + {x_int}:C(SDI_Q)\")\n",
    "            else:\n",
    "                rhs.append(f\"{x} * C(SDI_Q)\")\n",
    "        else:\n",
    "            rhs.append(f\"{x}\")\n",
    "    rhs += controls\n",
    "    return \"log_lui_rate_mean ~ \" + \" + \".join(rhs)\n",
    "\n",
    "def inter_terms(inter_set, cats, center_inter):\n",
    "    terms = []\n",
    "    for x in inter_set:\n",
    "        x_int = f\"{x}_CQ\" if center_inter else x\n",
    "        for g in cats[1:]:\n",
    "            terms.append(f\"{x_int}:C(SDI_Q)[T.{g}]\")\n",
    "    return terms\n",
    "\n",
    "def wald_pvalue(res, terms):\n",
    "    if not terms:\n",
    "        return np.nan\n",
    "    params = res.params.index.tolist()\n",
    "    keep = [t for t in terms if t in params]\n",
    "    if not keep:\n",
    "        return np.nan\n",
    "    R = np.zeros((len(keep), len(params)))\n",
    "    for i, t in enumerate(keep):\n",
    "        R[i, params.index(t)] = 1.0\n",
    "    try:\n",
    "        wt = res.wald_test(R)\n",
    "        return float(np.asarray(wt.pvalue).ravel()[0])\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "# ‚úÖÊñ∞Â¢ûÔºöÂõõ‰∏ªÊïàÂ∫îÊòæËëóÊÄßÂà§Âà´\n",
    "def main4_pvals_all_sig(res, exposures, p_cap=0.05):\n",
    "    \"\"\"\n",
    "    Ê£ÄÊü•Âõõ‰∏™Êö¥Èú≤‰∏ªÊïàÂ∫îÔºàHAP_X, PM25_X, CLIM1_X, CLIM2_XÔºâÊòØÂê¶ÈÉΩÊòæËëó„ÄÇ\n",
    "    ËøîÂõûÔºöall_sig(bool), pvals(dict)\n",
    "    \"\"\"\n",
    "    p = res.pvalues\n",
    "    pvals = {}\n",
    "    ok = True\n",
    "    for x in exposures:\n",
    "        px = float(p.get(x, np.nan))\n",
    "        pvals[x] = px\n",
    "        if (not np.isfinite(px)) or (px >= p_cap):\n",
    "            ok = False\n",
    "    return ok, pvals\n",
    "\n",
    "def cv_rmse(formula, d_raw, exposures, q=5, mergeTop=False, center_inter=False, k=5, seed=123):\n",
    "    y = d_raw[\"log_lui_rate_mean\"].to_numpy()\n",
    "    folds = kfold_pos_indices(len(d_raw), k=k, seed=seed)\n",
    "    preds = np.full_like(y, np.nan, dtype=float)\n",
    "\n",
    "    all_pos = np.arange(len(d_raw), dtype=int)\n",
    "\n",
    "    for test_pos in folds:\n",
    "        train_pos = np.setdiff1d(all_pos, test_pos)\n",
    "\n",
    "        dd_full = add_sdi_q_foldwise(d_raw, q=q, mergeTop=mergeTop, train_pos=train_pos)\n",
    "\n",
    "        train = dd_full[dd_full[\"rid\"].isin(train_pos)].copy()\n",
    "        test  = dd_full[dd_full[\"rid\"].isin(test_pos)].copy()\n",
    "\n",
    "        if len(train) < 30 or len(test) < 5:\n",
    "            continue\n",
    "\n",
    "        train = add_center_within_q(train, exposures, center_inter)\n",
    "        test  = add_center_within_q(test, exposures, center_inter)\n",
    "\n",
    "        try:\n",
    "            m = smf.ols(formula, data=train).fit()\n",
    "            pr = m.predict(test)\n",
    "            preds[test[\"rid\"].to_numpy().astype(int)] = pr\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    ok = np.isfinite(preds)\n",
    "    if ok.sum() < max(10, len(d_raw)//3):\n",
    "        return np.nan\n",
    "    return float(np.sqrt(np.mean((preds[ok] - y[ok])**2)))\n",
    "\n",
    "# =========================\n",
    "# BUILD DATASET (4 EXPO ALWAYS)\n",
    "# =========================\n",
    "def build_dataset_4expo(df_mean, hap_log1p, pm25_log1p, zall, clim_struct):\n",
    "    d = df_mean.copy()\n",
    "\n",
    "    d[\"HAP_X\"]  = safe_log1p(d[\"hap_mean\"])  if hap_log1p  else d[\"hap_mean\"]\n",
    "    d[\"PM25_X\"] = safe_log1p(d[\"pm25_mean\"]) if pm25_log1p else d[\"pm25_mean\"]\n",
    "    d[\"TAVG_X\"] = d[\"tavg_mean\"]\n",
    "    d[\"AH_X\"]   = d[\"ah_mean\"]\n",
    "    d[\"DENS_X\"] = safe_log1p(d[\"dens_mean\"])\n",
    "\n",
    "    if clim_struct == \"both_raw\":\n",
    "        d[\"CLIM1_X\"] = d[\"TAVG_X\"]\n",
    "        d[\"CLIM2_X\"] = d[\"AH_X\"]\n",
    "    elif clim_struct == \"both_AHresid_on_TAVG\":\n",
    "        tmp = d.dropna(subset=[\"AH_X\",\"TAVG_X\"]).copy()\n",
    "        m = smf.ols(\"AH_X ~ TAVG_X\", data=tmp).fit()\n",
    "        d[\"AH_resid\"] = d[\"AH_X\"] - m.predict(d)\n",
    "        d[\"CLIM1_X\"] = d[\"TAVG_X\"]\n",
    "        d[\"CLIM2_X\"] = d[\"AH_resid\"]\n",
    "    elif clim_struct == \"both_TAVGresid_on_AH\":\n",
    "        tmp = d.dropna(subset=[\"AH_X\",\"TAVG_X\"]).copy()\n",
    "        m = smf.ols(\"TAVG_X ~ AH_X\", data=tmp).fit()\n",
    "        d[\"TAVG_resid\"] = d[\"TAVG_X\"] - m.predict(d)\n",
    "        d[\"CLIM1_X\"] = d[\"AH_X\"]\n",
    "        d[\"CLIM2_X\"] = d[\"TAVG_resid\"]\n",
    "    else:\n",
    "        raise ValueError(\"Unknown clim_struct\")\n",
    "\n",
    "    if zall:\n",
    "        for c in [\"HAP_X\",\"PM25_X\",\"CLIM1_X\",\"CLIM2_X\",\"DENS_X\"]:\n",
    "            d[c] = zscore(d[c])\n",
    "\n",
    "    need = [\"log_lui_rate_mean\",\"sdi_mean\",\"HAP_X\",\"PM25_X\",\"CLIM1_X\",\"CLIM2_X\",\"DENS_X\"]\n",
    "    d = d.dropna(subset=need).copy()\n",
    "\n",
    "    exposures = [\"HAP_X\",\"PM25_X\",\"CLIM1_X\",\"CLIM2_X\"]  # Ê∞∏Ëøú4Êö¥Èú≤\n",
    "    controls  = [\"DENS_X\"]\n",
    "    return d, exposures, controls\n",
    "\n",
    "\n",
    "# =========================\n",
    "# ONE-AGE EXHAUST RUN\n",
    "# =========================\n",
    "def run_exhaust_one_age(df_mean, out_dir, vif_cap=20.0, verbose=True):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # ËÆæËÆ°Á©∫Èó¥\n",
    "    PIPE_HAP  = [(\"hap_raw\", False), (\"hap_log1p\", True)]\n",
    "    PIPE_PM25 = [(\"pm25_raw\", False), (\"pm25_log1p\", True)]\n",
    "    PIPE_ZALL = [(\"noz\", False), (\"zall\", True)]\n",
    "\n",
    "    CLIM_STRUCTS = [\"both_raw\", \"both_AHresid_on_TAVG\", \"both_TAVGresid_on_AH\"]\n",
    "\n",
    "    SDI_SCHEMES = [(\"Q5\", 5, False), (\"Q4_mergeTop\", 5, True)]\n",
    "    CODINGS = [(\"modifier_only\", True), (\"star_expand\", False)]\n",
    "    CENTER_OPTS = [(\"nocenter\", False), (\"centerWithinQ\", True)]\n",
    "\n",
    "    # ‰øùÂ≠ò base Âø´ÁÖß\n",
    "    base_fp = os.path.join(out_dir, \"iso3_longterm_means_base.csv\")\n",
    "    df_mean.to_csv(base_fp, index=False, encoding=\"utf-8-sig\")\n",
    "    if verbose:\n",
    "        print(\"Saved:\", base_fp, \"| n_iso3:\", df_mean[\"iso3\"].nunique())\n",
    "\n",
    "    N_ISO = int(df_mean[\"iso3\"].nunique())\n",
    "\n",
    "    ALL = []\n",
    "    job_id = 0\n",
    "\n",
    "    for hap_name, hap_log1p in PIPE_HAP:\n",
    "        for pm_name, pm25_log1p in PIPE_PM25:\n",
    "            for z_name, zall in PIPE_ZALL:\n",
    "                for clim_struct in CLIM_STRUCTS:\n",
    "\n",
    "                    d_raw, exposures, controls = build_dataset_4expo(df_mean, hap_log1p, pm25_log1p, zall, clim_struct)\n",
    "                    d_raw = d_raw.reset_index(drop=True).copy()\n",
    "                    d_raw[\"rid\"] = np.arange(len(d_raw), dtype=int)\n",
    "\n",
    "                    for sdi_scheme, q, mergeTop in SDI_SCHEMES:\n",
    "                        d_fit = add_sdi_q_foldwise(d_raw, q=q, mergeTop=mergeTop, train_pos=None)\n",
    "\n",
    "                        for coding_name, modifier_only in CODINGS:\n",
    "                            for center_name, center_inter in CENTER_OPTS:\n",
    "                                d0 = add_center_within_q(d_fit, exposures, center_inter)\n",
    "\n",
    "                                try:\n",
    "                                    cats = list(d0[\"SDI_Q\"].cat.categories)\n",
    "                                except Exception:\n",
    "                                    cats = None\n",
    "\n",
    "                                for r in range(0, len(exposures)+1):\n",
    "                                    for comb in itertools.combinations(exposures, r):\n",
    "                                        inter_set = set(comb)\n",
    "                                        tag = \"+\".join(sorted(inter_set)) if inter_set else \"NO_INTER\"\n",
    "                                        formula = build_formula(exposures, controls, inter_set, modifier_only, center_inter)\n",
    "\n",
    "                                        job_id += 1\n",
    "\n",
    "                                        aic=bic=adjr2=cv=waldp=max_vif=np.nan\n",
    "                                        fit_ok=0\n",
    "                                        err_main=err_cv=err_vif=err_wald=\"\"\n",
    "                                        main4_all_sig = 0\n",
    "                                        p_HAP=p_PM25=p_C1=p_C2 = np.nan\n",
    "\n",
    "                                        # main fit\n",
    "                                        try:\n",
    "                                            res = smf.ols(formula, data=d0).fit(cov_type=SE_TYPE)\n",
    "                                            fit_ok = 1\n",
    "                                            aic = float(res.aic) if np.isfinite(res.aic) else np.nan\n",
    "                                            bic = float(res.bic) if np.isfinite(res.bic) else np.nan\n",
    "                                            adjr2 = float(res.rsquared_adj) if np.isfinite(res.rsquared_adj) else np.nan\n",
    "\n",
    "                                            # ‚úÖÊñ∞Â¢ûÔºöÂõõ‰∏ªÊïàÂ∫îÊòæËëóÊÄß\n",
    "                                            all_sig, p_main = main4_pvals_all_sig(res, exposures, p_cap=P_MAIN_CAP)\n",
    "                                            main4_all_sig = int(all_sig)\n",
    "                                            p_HAP  = p_main.get(\"HAP_X\", np.nan)\n",
    "                                            p_PM25 = p_main.get(\"PM25_X\", np.nan)\n",
    "                                            p_C1   = p_main.get(\"CLIM1_X\", np.nan)\n",
    "                                            p_C2   = p_main.get(\"CLIM2_X\", np.nan)\n",
    "\n",
    "                                        except Exception as e:\n",
    "                                            err_main = str(e)[:200]\n",
    "                                            ALL.append({\n",
    "                                                \"job_id\": job_id,\n",
    "                                                \"hap\": hap_name, \"pm25\": pm_name, \"zall\": z_name,\n",
    "                                                \"clim_struct\": clim_struct,\n",
    "                                                \"sdi_scheme\": sdi_scheme,\n",
    "                                                \"coding\": coding_name,\n",
    "                                                \"center_inter\": center_name,\n",
    "                                                \"interactions\": tag,\n",
    "                                                \"n_fit\": int(len(d0)),\n",
    "                                                \"formula\": formula,\n",
    "                                                \"AIC\": aic, \"BIC\": bic, \"AdjR2\": adjr2,\n",
    "                                                \"CV_RMSE\": cv, \"WaldP_inter\": waldp, \"maxVIF\": max_vif,\n",
    "                                                \"main4_all_sig\": int(main4_all_sig),\n",
    "                                                \"p_HAP_main\": p_HAP,\n",
    "                                                \"p_PM25_main\": p_PM25,\n",
    "                                                \"p_CLIM1_main\": p_C1,\n",
    "                                                \"p_CLIM2_main\": p_C2,\n",
    "                                                \"fit_ok\": fit_ok,\n",
    "                                                \"err_main\": err_main, \"err_cv\": err_cv, \"err_vif\": err_vif, \"err_wald\": err_wald,\n",
    "                                            })\n",
    "                                            continue\n",
    "\n",
    "                                        # CV\n",
    "                                        try:\n",
    "                                            cv = cv_rmse(formula, d_raw, exposures, q=q, mergeTop=mergeTop,\n",
    "                                                         center_inter=center_inter, k=CV_K, seed=SEED)\n",
    "                                        except Exception as e:\n",
    "                                            err_cv = str(e)[:200]\n",
    "                                            cv = np.nan\n",
    "\n",
    "                                        # Wald\n",
    "                                        try:\n",
    "                                            if cats is not None:\n",
    "                                                terms = inter_terms(inter_set, cats, center_inter)\n",
    "                                                waldp = wald_pvalue(res, terms)\n",
    "                                        except Exception as e:\n",
    "                                            err_wald = str(e)[:200]\n",
    "                                            waldp = np.nan\n",
    "\n",
    "                                        # VIF\n",
    "                                        try:\n",
    "                                            vif_df = compute_vif_exog(res.model.exog, res.model.exog_names)\n",
    "                                            max_vif = float(np.nanmax(vif_df[\"VIF\"].to_numpy())) if len(vif_df) else np.nan\n",
    "                                        except Exception as e:\n",
    "                                            err_vif = str(e)[:200]\n",
    "                                            max_vif = np.nan\n",
    "\n",
    "                                        ALL.append({\n",
    "                                            \"job_id\": job_id,\n",
    "                                            \"hap\": hap_name, \"pm25\": pm_name, \"zall\": z_name,\n",
    "                                            \"clim_struct\": clim_struct,\n",
    "                                            \"sdi_scheme\": sdi_scheme,\n",
    "                                            \"coding\": coding_name,\n",
    "                                            \"center_inter\": center_name,\n",
    "                                            \"interactions\": tag,\n",
    "                                            \"n_fit\": int(len(d0)),\n",
    "                                            \"formula\": formula,\n",
    "                                            \"AIC\": aic, \"BIC\": bic, \"AdjR2\": adjr2,\n",
    "                                            \"CV_RMSE\": cv, \"WaldP_inter\": waldp, \"maxVIF\": max_vif,\n",
    "                                            \"main4_all_sig\": int(main4_all_sig),\n",
    "                                            \"p_HAP_main\": p_HAP,\n",
    "                                            \"p_PM25_main\": p_PM25,\n",
    "                                            \"p_CLIM1_main\": p_C1,\n",
    "                                            \"p_CLIM2_main\": p_C2,\n",
    "                                            \"fit_ok\": fit_ok,\n",
    "                                            \"err_main\": err_main, \"err_cv\": err_cv, \"err_vif\": err_vif, \"err_wald\": err_wald,\n",
    "                                        })\n",
    "\n",
    "    df_res = pd.DataFrame(ALL)\n",
    "    df_res[\"AdjR2\"] = pd.to_numeric(df_res[\"AdjR2\"], errors=\"coerce\")\n",
    "    df_res[\"AdjR2_neg\"] = -df_res[\"AdjR2\"]\n",
    "    for c in [\"CV_RMSE\",\"BIC\",\"AdjR2_neg\",\"AIC\",\"maxVIF\",\"WaldP_inter\",\n",
    "              \"p_HAP_main\",\"p_PM25_main\",\"p_CLIM1_main\",\"p_CLIM2_main\",\"main4_all_sig\"]:\n",
    "        if c in df_res.columns:\n",
    "            df_res[c] = pd.to_numeric(df_res[c], errors=\"coerce\")\n",
    "\n",
    "    res_fp = os.path.join(out_dir, \"FULL_EXHAUST_4EXPO_results_all_models.csv\")\n",
    "    df_res.to_csv(res_fp, index=False, encoding=\"utf-8-sig\")\n",
    "    if verbose:\n",
    "        print(\"Saved:\", res_fp)\n",
    "        print(\"Total jobs:\", len(df_res), \"| fit_ok:\", int(df_res[\"fit_ok\"].sum()))\n",
    "\n",
    "    # pick best (stable)\n",
    "    df_ok = df_res[df_res[\"fit_ok\"]==1].copy()\n",
    "    df_ok = df_ok.dropna(subset=[\"CV_RMSE\",\"BIC\",\"AdjR2_neg\"], how=\"any\").copy()\n",
    "    df_ok = df_ok[df_ok[\"n_fit\"] == N_ISO].copy()\n",
    "\n",
    "    # ‚úÖÊñ∞Â¢ûÔºöÂõõ‰∏ªÊïàÂ∫îÂøÖÈ°ªÊòæËëó\n",
    "    df_ok = df_ok[df_ok[\"main4_all_sig\"] == 1].copy()\n",
    "\n",
    "    df_stable = df_ok[np.isfinite(df_ok[\"maxVIF\"]) & (df_ok[\"maxVIF\"] <= float(vif_cap))].copy()\n",
    "    if len(df_stable) == 0:\n",
    "        # ÊîæÂÆΩÂà∞50ÔºåÈÅøÂÖçÊó†Ëß£\n",
    "        df_stable = df_ok[np.isfinite(df_ok[\"maxVIF\"]) & (df_ok[\"maxVIF\"] <= 50)].copy()\n",
    "\n",
    "    if len(df_stable) == 0:\n",
    "        raise RuntimeError(\n",
    "            \"No stable model found after requiring 4-main p<cap. \"\n",
    "            \"Try relax P_MAIN_CAP (e.g., 0.10) or relax VIF_CAP, or check data.\"\n",
    "        )\n",
    "\n",
    "    df_stable = df_stable.sort_values([\"CV_RMSE\",\"BIC\",\"AdjR2_neg\"], ascending=[True, True, True]).copy()\n",
    "    best = df_stable.iloc[0].to_dict()\n",
    "\n",
    "    best_fp = os.path.join(out_dir, \"BEST_model_row.csv\")\n",
    "    pd.DataFrame([best]).to_csv(best_fp, index=False, encoding=\"utf-8-sig\")\n",
    "    if verbose:\n",
    "        print(\"Saved:\", best_fp)\n",
    "        print(\"[BEST MODEL (stable, main4 p<cap)]\")\n",
    "        for k in [\"hap\",\"pm25\",\"zall\",\"clim_struct\",\"sdi_scheme\",\"coding\",\"center_inter\",\"interactions\",\n",
    "                  \"CV_RMSE\",\"BIC\",\"AdjR2\",\"WaldP_inter\",\"maxVIF\",\"n_fit\",\n",
    "                  \"main4_all_sig\",\"p_HAP_main\",\"p_PM25_main\",\"p_CLIM1_main\",\"p_CLIM2_main\",\n",
    "                  \"formula\"]:\n",
    "            print(f\"  {k}: {best.get(k)}\")\n",
    "\n",
    "    # refit best to export summary/coef/vif and slope plot\n",
    "    hap_log1p = (best[\"hap\"] == \"hap_log1p\")\n",
    "    pm25_log1p = (best[\"pm25\"] == \"pm25_log1p\")\n",
    "    zall = (best[\"zall\"] == \"zall\")\n",
    "    clim_struct = best[\"clim_struct\"]\n",
    "    mergeTop = (best[\"sdi_scheme\"] == \"Q4_mergeTop\")\n",
    "    center_inter = (best[\"center_inter\"] == \"centerWithinQ\")\n",
    "\n",
    "    d_raw_best, exposures, controls = build_dataset_4expo(df_mean, hap_log1p, pm25_log1p, zall, clim_struct)\n",
    "    d_raw_best = d_raw_best.reset_index(drop=True).copy()\n",
    "    d_raw_best[\"rid\"] = np.arange(len(d_raw_best), dtype=int)\n",
    "\n",
    "    d_fit_best = add_sdi_q_foldwise(d_raw_best, q=5, mergeTop=mergeTop, train_pos=None)\n",
    "    d_fit_best = add_center_within_q(d_fit_best, exposures, center_inter)\n",
    "\n",
    "    best_formula = best[\"formula\"]\n",
    "    res_best = smf.ols(best_formula, data=d_fit_best).fit(cov_type=SE_TYPE)\n",
    "\n",
    "    # summary\n",
    "    summ_fp = os.path.join(out_dir, \"BEST_model_summary.txt\")\n",
    "    with open(summ_fp, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"BEST FORMULA:\\n\" + best_formula + \"\\n\\n\")\n",
    "        f.write(f\"\\n[Main-4 p-value threshold] P_MAIN_CAP={P_MAIN_CAP}\\n\")\n",
    "        f.write(\"Main-4 p-values:\\n\")\n",
    "        for x in exposures:\n",
    "            f.write(f\"  {x}: {float(res_best.pvalues.get(x, np.nan))}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(res_best.summary().as_text())\n",
    "    if verbose:\n",
    "        print(\"Saved:\", summ_fp)\n",
    "\n",
    "    # coef\n",
    "    coef = pd.DataFrame({\n",
    "        \"term\": res_best.params.index,\n",
    "        \"beta\": res_best.params.values,\n",
    "        \"se\": res_best.bse.values,\n",
    "        \"p\": res_best.pvalues.values\n",
    "    }).sort_values(\"p\", ascending=True)\n",
    "\n",
    "    coef_fp = os.path.join(out_dir, \"BEST_model_coef.csv\")\n",
    "    coef.to_csv(coef_fp, index=False, encoding=\"utf-8-sig\")\n",
    "    if verbose:\n",
    "        print(\"Saved:\", coef_fp)\n",
    "\n",
    "    # VIF\n",
    "    try:\n",
    "        vif_df = compute_vif_exog(res_best.model.exog, res_best.model.exog_names)\n",
    "        vif_fp = os.path.join(out_dir, \"BEST_model_VIF.csv\")\n",
    "        vif_df.to_csv(vif_fp, index=False, encoding=\"utf-8-sig\")\n",
    "        if verbose:\n",
    "            print(\"Saved:\", vif_fp)\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(\"VIF failed:\", str(e)[:200])\n",
    "\n",
    "    # slope plot for interacted exposures\n",
    "    try:\n",
    "        cats = list(d_fit_best[\"SDI_Q\"].cat.categories)\n",
    "    except Exception:\n",
    "        cats = None\n",
    "\n",
    "    def slope_by_group(res, x, cats, center_inter=False):\n",
    "        b = res.params\n",
    "        V = res.cov_params()\n",
    "        x_int = f\"{x}_CQ\" if center_inter else x\n",
    "\n",
    "        out = []\n",
    "        g0 = cats[0]\n",
    "        b0 = float(b.get(x, np.nan))\n",
    "        v0 = float(V.loc[x, x]) if (x in V.index) else np.nan\n",
    "        se0 = float(np.sqrt(max(v0, 0.0))) if np.isfinite(v0) else np.nan\n",
    "        out.append((g0, b0, se0))\n",
    "\n",
    "        for g in cats[1:]:\n",
    "            term = f\"{x_int}:C(SDI_Q)[T.{g}]\"\n",
    "            if (x in b.index) and (term in b.index):\n",
    "                bg = float(b[x] + b[term])\n",
    "                var = float(V.loc[x, x] + V.loc[term, term] + 2.0*V.loc[x, term])\n",
    "                seg = float(np.sqrt(max(var, 0.0)))\n",
    "            else:\n",
    "                bg, seg = np.nan, np.nan\n",
    "            out.append((g, bg, seg))\n",
    "\n",
    "        df_s = pd.DataFrame(out, columns=[\"SDI_Q\",\"slope\",\"se\"])\n",
    "        df_s[\"ci_lo\"] = df_s[\"slope\"] - 1.96*df_s[\"se\"]\n",
    "        df_s[\"ci_hi\"] = df_s[\"slope\"] + 1.96*df_s[\"se\"]\n",
    "        return df_s\n",
    "\n",
    "    params = set(res_best.params.index.tolist())\n",
    "    interacted = []\n",
    "    if cats is not None:\n",
    "        for x in exposures:\n",
    "            x_int = f\"{x}_CQ\" if center_inter else x\n",
    "            has_any = any((f\"{x_int}:C(SDI_Q)[T.{g}]\" in params) for g in cats[1:])\n",
    "            if has_any:\n",
    "                interacted.append(x)\n",
    "\n",
    "    if cats is not None and interacted:\n",
    "        for x in interacted:\n",
    "            df_s = slope_by_group(res_best, x, cats, center_inter=center_inter)\n",
    "            out_csv = os.path.join(out_dir, f\"BEST_slope_by_SDI__{x}.csv\")\n",
    "            df_s.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "            plt.figure(figsize=(7.2, 4.6))\n",
    "            xx = np.arange(1, len(cats)+1)\n",
    "            plt.plot(xx, df_s[\"slope\"].values, marker=\"o\", lw=2.2)\n",
    "            plt.fill_between(xx, df_s[\"ci_lo\"].values, df_s[\"ci_hi\"].values, alpha=0.18)\n",
    "            plt.axhline(0, lw=1, color=\"k\", alpha=0.6)\n",
    "            plt.xticks(xx, df_s[\"SDI_Q\"].astype(str).values)\n",
    "            plt.xlabel(\"SDI group\")\n",
    "            plt.ylabel(f\"Slope of {x} on log(mean LUI rate)\")\n",
    "            plt.title(f\"BEST model: {x} slope across SDI groups\")\n",
    "            plt.tight_layout()\n",
    "\n",
    "            out_png = os.path.join(out_dir, f\"BEST_slope_{x}_by_SDI.png\")\n",
    "            out_pdf = os.path.join(out_dir, f\"BEST_slope_{x}_by_SDI.pdf\")\n",
    "            plt.savefig(out_png, dpi=300, bbox_inches=\"tight\")\n",
    "            plt.savefig(out_pdf, bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "\n",
    "    return best, df_res\n",
    "\n",
    "\n",
    "# =========================\n",
    "# MAIN: LOAD PANEL ONCE, RUN ALL AGES\n",
    "# =========================\n",
    "def main():\n",
    "    print(\"Loading panel:\", IN_FP)\n",
    "    df0 = pd.read_csv(IN_FP)\n",
    "\n",
    "    df0[\"iso3\"] = df0[\"iso3\"].astype(str).str.upper().str.strip()\n",
    "    df0[\"year\"] = pd.to_numeric(df0[\"year\"], errors=\"coerce\")\n",
    "    df0 = df0[(df0[\"year\"] >= Y0) & (df0[\"year\"] <= Y1)].copy()\n",
    "\n",
    "    best_rows = []\n",
    "\n",
    "    for age, spec in AGE_SPECS.items():\n",
    "        y_col = spec[\"y\"]\n",
    "        pop_col = spec[\"pop\"]\n",
    "\n",
    "        out_dir = os.path.join(OUT_ROOT, f\"AGE_{age}\")\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*90)\n",
    "        print(f\"üöÄ AGE={age} | y={y_col} | pop={pop_col}\")\n",
    "        print(\"=\"*90)\n",
    "\n",
    "        need = [\n",
    "            \"iso3\",\"year\", y_col, pop_col,\n",
    "            HAP_COL, PM25_COL, TAVG_COL, AH_COL,\n",
    "            SDI_COL, DENS_COL\n",
    "        ]\n",
    "        require_cols(df0, need)\n",
    "\n",
    "        df = df0.dropna(subset=need).copy()\n",
    "        df[y_col] = pd.to_numeric(df[y_col], errors=\"coerce\")\n",
    "        df[pop_col] = pd.to_numeric(df[pop_col], errors=\"coerce\")\n",
    "\n",
    "        df = df.dropna(subset=[y_col, pop_col]).copy()\n",
    "        df[\"rate\"] = df[y_col] / df[pop_col].clip(lower=1.0) * 100_000.0\n",
    "\n",
    "        df_mean = (\n",
    "            df.groupby(\"iso3\", as_index=False)\n",
    "              .agg(\n",
    "                  lui_rate_mean=(\"rate\",\"mean\"),\n",
    "                  hap_mean=(HAP_COL,\"mean\"),\n",
    "                  pm25_mean=(PM25_COL,\"mean\"),\n",
    "                  tavg_mean=(TAVG_COL,\"mean\"),\n",
    "                  ah_mean=(AH_COL,\"mean\"),\n",
    "                  sdi_mean=(SDI_COL,\"mean\"),\n",
    "                  dens_mean=(DENS_COL,\"mean\"),\n",
    "                  n_years=(\"year\",\"nunique\"),\n",
    "              )\n",
    "        )\n",
    "        df_mean[\"log_lui_rate_mean\"] = safe_log(df_mean[\"lui_rate_mean\"], floor=1e-6)\n",
    "\n",
    "        # Ë∑ëÁ©∑‰∏æ\n",
    "        best, _ = run_exhaust_one_age(df_mean, out_dir, vif_cap=VIF_CAP, verbose=True)\n",
    "\n",
    "        best_row = best.copy()\n",
    "        best_row[\"age\"] = age\n",
    "        best_row[\"y_col\"] = y_col\n",
    "        best_row[\"pop_col\"] = pop_col\n",
    "        best_rows.append(best_row)\n",
    "\n",
    "    # Ê±áÊÄª\n",
    "    df_best = pd.DataFrame(best_rows)\n",
    "    out_fp = os.path.join(OUT_ROOT, \"BEST_MODELS_ALL_AGES.csv\")\n",
    "    df_best.to_csv(out_fp, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(\"\\nüéØ DONE. Summary saved:\")\n",
    "    print(out_fp)\n",
    "    show_cols = [\"age\",\"formula\",\"CV_RMSE\",\"BIC\",\"AdjR2\",\"WaldP_inter\",\"maxVIF\",\"interactions\",\n",
    "                 \"coding\",\"center_inter\",\"clim_struct\",\"hap\",\"pm25\",\"zall\",\"sdi_scheme\",\n",
    "                 \"main4_all_sig\",\"p_HAP_main\",\"p_PM25_main\",\"p_CLIM1_main\",\"p_CLIM2_main\"]\n",
    "    show_cols = [c for c in show_cols if c in df_best.columns]\n",
    "    print(df_best[show_cols])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e972526-eb86-4e02-a8be-44a85bfb61bd",
   "metadata": {},
   "source": [
    " ## model used in this issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb2403f-73a0-463d-8fa4-b6e122e49c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "from patsy import dmatrix, build_design_matrices\n",
    "\n",
    "import geopandas as gpd\n",
    "import cartopy.crs as ccrs\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# STYLE\n",
    "# =========================\n",
    "mpl.rcParams[\"pdf.fonttype\"] = 42\n",
    "mpl.rcParams[\"ps.fonttype\"]  = 42\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"Arial\", \"SimHei\", \"DejaVu Sans\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "# =========================\n",
    "# PATHS\n",
    "# =========================\n",
    "IN_FP  = r\"D:\\AAUDE\\paper_v2\\paper2\\data\\model_outputs\\panel0_1990_2019_direct_meteo_GBDPM_HAP_lui.csv\"\n",
    "SHP_FP = r\"D:\\AAUDE\\paper_v2\\paper2\\data\\ne_10m_admin_0_countries\\ne_10m_admin_0_countries.shp\"\n",
    "OUT_ROOT = r\"D:\\AAUDE\\paper_v2\\paper2\\data\\model_outputs\\FIG_AIR_2x2_ALL_AGES_IMPACT_PCT_Q5PLOT_CI_HATCH_v2\"\n",
    "os.makedirs(OUT_ROOT, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# GLOBAL CONFIG\n",
    "# =========================\n",
    "Y0, Y1 = 1990, 2019\n",
    "MAP_YEAR = 2019\n",
    "RATE_PER = 100_000.0\n",
    "\n",
    "HAP_RAW  = \"hap_pm_pw\"\n",
    "PM25_RAW = \"pm25_pw\"\n",
    "TAVG_RAW = \"tavg_pw_C\"\n",
    "AH_RAW   = \"ah_pw\"\n",
    "DENS_RAW = \"density_total_pkm2\"\n",
    "SDI_RAW  = \"sdi\"\n",
    "\n",
    "AUTO_SCALE_DIV10 = False\n",
    "\n",
    "TMREL_HAP_BASE  = 2.4\n",
    "TMREL_PM25_BASE = 2.4\n",
    "\n",
    "# SDI colors (Q1..Q5)\n",
    "SDI_LABELS_Q5 = [\"Q1\",\"Q2\",\"Q3\",\"Q4\",\"Q5\"]\n",
    "SDI_HEX_Q5 = [ \"#547BB4\", \"#DD7C4F\",\"#629C35\", \"#C0321A\", \"#6C61AF\"]\n",
    "\n",
    "# ---- CI simulation ----\n",
    "SIM_N = 3000\n",
    "SIM_SEED = 123\n",
    "CI_LO, CI_HI = 2.5, 97.5\n",
    "\n",
    "# =========================\n",
    "# AGE-SPECIFIC BEST SETTINGS\n",
    "# =========================\n",
    "AGE_CFG = {\n",
    "    \"total\": dict(\n",
    "        y=\"uri_total\", pop=\"pop_total\",\n",
    "        hap=\"hap_log1p\",\n",
    "        pm25=\"pm25_raw\",\n",
    "        sdi_scheme=\"Q4_mergeTop\",\n",
    "        clim_struct=\"both_TAVGresid_on_AH\",\n",
    "        center_inter=\"nocenter\",\n",
    "        rhs=\"HAP_X + PM25_X + PM25_X:C(SDI_Q) + CLIM1_X + CLIM2_X + DENS_X\",\n",
    "        title_tag=\"AGE_total\"\n",
    "    ),\n",
    "    \"u5\": dict(\n",
    "        y=\"uri_u5\", pop=\"pop_u5\",\n",
    "        hap=\"hap_log1p\",\n",
    "        pm25=\"pm25_log1p\",\n",
    "        sdi_scheme=\"Q4_mergeTop\",\n",
    "        clim_struct=\"both_TAVGresid_on_AH\",\n",
    "        center_inter=\"nocenter\",\n",
    "        rhs=\"HAP_X + PM25_X * C(SDI_Q) + CLIM1_X * C(SDI_Q) + CLIM2_X * C(SDI_Q) + DENS_X\",\n",
    "        title_tag=\"AGE_u5\"\n",
    "    ),\n",
    "    \"5_65\": dict(\n",
    "        y=\"uri_5_65\", pop=\"pop_5_65\",\n",
    "        hap=\"hap_raw\",\n",
    "        pm25=\"pm25_raw\",\n",
    "        sdi_scheme=\"Q5\",\n",
    "        clim_struct=\"both_TAVGresid_on_AH\",\n",
    "        center_inter=\"nocenter\",\n",
    "        rhs=\"HAP_X + HAP_X:C(SDI_Q) + PM25_X + CLIM1_X + CLIM2_X + DENS_X\",\n",
    "        title_tag=\"AGE_5_65\"\n",
    "    ),\n",
    "    \"65p\": dict(\n",
    "        y=\"uri_65p\", pop=\"pop_65p\",\n",
    "        hap=\"hap_log1p\",\n",
    "        pm25=\"pm25_raw\",\n",
    "        sdi_scheme=\"Q4_mergeTop\",\n",
    "        clim_struct=\"both_TAVGresid_on_AH\",\n",
    "        center_inter=\"nocenter\",\n",
    "        rhs=\"HAP_X + PM25_X + PM25_X:C(SDI_Q) + CLIM1_X + CLIM2_X + DENS_X\",\n",
    "        title_tag=\"AGE_65p\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# helpers\n",
    "# =========================\n",
    "def require_cols(df, cols):\n",
    "    miss = [c for c in cols if c not in df.columns]\n",
    "    if miss:\n",
    "        raise ValueError(f\"Missing columns: {miss}\")\n",
    "\n",
    "def safe_log(x, floor=1e-12):\n",
    "    return np.log(np.clip(x, floor, None))\n",
    "\n",
    "def safe_log1p(x):\n",
    "    return np.log1p(np.clip(x, 0.0, None))\n",
    "\n",
    "def load_world(shp_fp):\n",
    "    g = gpd.read_file(shp_fp)\n",
    "    cand = [\"ADM0_A3\", \"ISO_A3\", \"SOV_A3\", \"WB_A3\", \"ISO3\", \"iso3\"]\n",
    "    key = None\n",
    "    for c in cand:\n",
    "        if c in g.columns:\n",
    "            key = c\n",
    "            break\n",
    "    if key is None:\n",
    "        raise ValueError(f\"Shapefile lacks iso3. Available: {list(g.columns)}\")\n",
    "    g = g.rename(columns={key: \"iso3\"})\n",
    "    g[\"iso3\"] = g[\"iso3\"].astype(str).str.upper().str.strip()\n",
    "    g.loc[g[\"iso3\"] == \"-99\", \"iso3\"] = np.nan\n",
    "    g = g.dropna(subset=[\"iso3\"]).copy()\n",
    "    g = g[g[\"iso3\"] != \"ATA\"].copy()\n",
    "    return g\n",
    "\n",
    "def build_iso3_means(df_panel, y_col, pop_col):\n",
    "    d = df_panel.copy()\n",
    "    d[\"rate\"] = d[y_col] / d[pop_col].clip(lower=1.0) * RATE_PER\n",
    "    df_mean = (\n",
    "        d.groupby(\"iso3\", as_index=False)\n",
    "         .agg(\n",
    "             lui_rate_mean=(\"rate\",\"mean\"),\n",
    "             hap_mean=(HAP_RAW,\"mean\"),\n",
    "             pm25_mean=(PM25_RAW,\"mean\"),\n",
    "             tavg_mean=(TAVG_RAW,\"mean\"),\n",
    "             ah_mean=(AH_RAW,\"mean\"),\n",
    "             sdi_mean=(SDI_RAW,\"mean\"),\n",
    "             dens_mean=(DENS_RAW,\"mean\"),\n",
    "         )\n",
    "    )\n",
    "    df_mean[\"log_lui_rate_mean\"] = safe_log(df_mean[\"lui_rate_mean\"], floor=1e-6)\n",
    "    return df_mean\n",
    "\n",
    "def make_sdi_maps_from_means(df_mean, scheme_model=\"Q4_mergeTop\"):\n",
    "    s = df_mean.set_index(\"iso3\")[\"sdi_mean\"].astype(float)\n",
    "\n",
    "    # plot Q5\n",
    "    labels5 = [f\"Q{i}\" for i in range(1, 6)]\n",
    "    q5 = pd.qcut(s, q=5, labels=labels5, duplicates=\"drop\").astype(str)\n",
    "    sdi_map_plot5 = q5.to_dict()\n",
    "    labels_plot5 = labels5\n",
    "\n",
    "    # model scheme\n",
    "    if scheme_model == \"Q5\":\n",
    "        sdi_map_model = sdi_map_plot5\n",
    "        labels_model = labels_plot5\n",
    "    elif scheme_model == \"Q4_mergeTop\":\n",
    "        q4 = q5.replace({\"Q4\": \"TOP\", \"Q5\": \"TOP\"})\n",
    "        sdi_map_model = q4.to_dict()\n",
    "        labels_model = [\"Q1\", \"Q2\", \"Q3\", \"TOP\"]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown scheme_model: {scheme_model}\")\n",
    "\n",
    "    return sdi_map_model, labels_model, sdi_map_plot5, labels_plot5\n",
    "\n",
    "def fit_resid_params_on_means(df_mean, clim_struct):\n",
    "    if clim_struct == \"both_raw\":\n",
    "        return None, None, \"RAW\"\n",
    "    if clim_struct == \"both_AHresid_on_TAVG\":\n",
    "        tmp = df_mean.dropna(subset=[\"ah_mean\",\"tavg_mean\"]).copy()\n",
    "        m = smf.ols(\"ah_mean ~ tavg_mean\", data=tmp).fit()\n",
    "        return float(m.params[\"Intercept\"]), float(m.params[\"tavg_mean\"]), \"AH_on_TAVG\"\n",
    "    if clim_struct == \"both_TAVGresid_on_AH\":\n",
    "        tmp = df_mean.dropna(subset=[\"ah_mean\",\"tavg_mean\"]).copy()\n",
    "        m = smf.ols(\"tavg_mean ~ ah_mean\", data=tmp).fit()\n",
    "        return float(m.params[\"Intercept\"]), float(m.params[\"ah_mean\"]), \"TAVG_on_AH\"\n",
    "    raise ValueError(f\"Unknown clim_struct: {clim_struct}\")\n",
    "\n",
    "def transform_hap_from_raw(x_raw, hap_mode):\n",
    "    x_raw = pd.to_numeric(x_raw, errors=\"coerce\").astype(float)\n",
    "    if hap_mode == \"hap_log1p\":\n",
    "        return safe_log1p(x_raw)\n",
    "    if hap_mode == \"hap_raw\":\n",
    "        return x_raw\n",
    "    raise ValueError(f\"Unknown hap mode: {hap_mode}\")\n",
    "\n",
    "def transform_pm25_from_raw(x_raw, pm_mode):\n",
    "    x_raw = pd.to_numeric(x_raw, errors=\"coerce\").astype(float)\n",
    "    if pm_mode == \"pm25_raw\":\n",
    "        return x_raw\n",
    "    if pm_mode == \"pm25_log1p\":\n",
    "        return safe_log1p(x_raw)\n",
    "    raise ValueError(f\"Unknown pm25 mode: {pm_mode}\")\n",
    "\n",
    "def apply_climate_struct(d, resid_params, is_means=True):\n",
    "    a0, b1, mode = resid_params\n",
    "    out = d.copy()\n",
    "    if is_means:\n",
    "        t = pd.to_numeric(out[\"tavg_mean\"], errors=\"coerce\")\n",
    "        a = pd.to_numeric(out[\"ah_mean\"], errors=\"coerce\")\n",
    "    else:\n",
    "        t = pd.to_numeric(out[\"TAVG\"], errors=\"coerce\")\n",
    "        a = pd.to_numeric(out[\"AH\"], errors=\"coerce\")\n",
    "\n",
    "    if mode == \"RAW\":\n",
    "        out[\"CLIM1_X\"] = t\n",
    "        out[\"CLIM2_X\"] = a\n",
    "    elif mode == \"AH_on_TAVG\":\n",
    "        out[\"CLIM1_X\"] = t\n",
    "        out[\"CLIM2_X\"] = a - (a0 + b1 * t)\n",
    "    elif mode == \"TAVG_on_AH\":\n",
    "        out[\"CLIM1_X\"] = t - (a0 + b1 * a)\n",
    "        out[\"CLIM2_X\"] = a\n",
    "    else:\n",
    "        raise ValueError(mode)\n",
    "    return out\n",
    "\n",
    "def interacted_exposures_from_rhs(rhs):\n",
    "    expos = [\"HAP_X\", \"PM25_X\", \"CLIM1_X\", \"CLIM2_X\"]\n",
    "    inter = set()\n",
    "    for x in expos:\n",
    "        if (f\"{x}:C(SDI_Q)\" in rhs) or (f\"{x}_CQ:C(SDI_Q)\" in rhs) or (f\"{x}*C(SDI_Q)\" in rhs) or (f\"{x} * C(SDI_Q)\" in rhs):\n",
    "            inter.add(x)\n",
    "    return inter\n",
    "\n",
    "def add_center_within_q(d, rhs, group_col=\"SDI_Q\"):\n",
    "    out = d.copy()\n",
    "    inter = interacted_exposures_from_rhs(rhs)\n",
    "    for x in sorted(inter):\n",
    "        mu = out.groupby(group_col)[x].transform(\"mean\")\n",
    "        out[f\"{x}_CQ\"] = out[x] - mu\n",
    "    return out\n",
    "\n",
    "def build_design(df, rhs, design_info=None):\n",
    "    if design_info is None:\n",
    "        X = dmatrix(\"1 + \" + rhs, data=df, return_type=\"dataframe\")\n",
    "        return X, X.design_info\n",
    "    mats = build_design_matrices([design_info], df, return_type=\"dataframe\")\n",
    "    return mats[0], design_info\n",
    "\n",
    "def predict_rate(beta, X):\n",
    "    eta = np.asarray(X.values @ beta, float)\n",
    "    return np.exp(eta)  # rate per 100k\n",
    "\n",
    "# =========================\n",
    "# CI via Monte Carlo (PSD clip)\n",
    "# =========================\n",
    "def draw_betas_psd(beta_hat, cov, n=1000, seed=123, eig_floor=1e-8):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    cov = np.asarray(cov, float)\n",
    "    cov = 0.5 * (cov + cov.T)\n",
    "\n",
    "    w, V = np.linalg.eigh(cov)\n",
    "    w = np.clip(w, eig_floor, None)\n",
    "    cov_psd = (V * w) @ V.T\n",
    "\n",
    "    return rng.multivariate_normal(mean=np.asarray(beta_hat, float), cov=cov_psd, size=n)\n",
    "\n",
    "def agg_pct_by_codes(ro, rc, codes, n_codes):\n",
    "    ro_sum = np.bincount(codes, weights=ro, minlength=n_codes).astype(float)\n",
    "    dr_sum = np.bincount(codes, weights=(ro - rc), minlength=n_codes).astype(float)\n",
    "    denom = np.maximum(ro_sum, 1e-12)\n",
    "    return dr_sum / denom * 100.0\n",
    "\n",
    "def impact_map_with_ci(beta_hat, cov, Xo, Xc, iso3_series, n_draw=1000, seed=123):\n",
    "    iso3 = iso3_series.astype(str).values\n",
    "    uniq = np.unique(iso3)\n",
    "    code_map = {k:i for i,k in enumerate(uniq)}\n",
    "    codes = np.array([code_map[k] for k in iso3], dtype=int)\n",
    "    m = len(uniq)\n",
    "\n",
    "    ro0 = predict_rate(beta_hat, Xo)\n",
    "    rc0 = predict_rate(beta_hat, Xc)\n",
    "    imp0 = agg_pct_by_codes(ro0, rc0, codes, m)\n",
    "\n",
    "    B = draw_betas_psd(beta_hat, cov, n=n_draw, seed=seed)\n",
    "    sims = np.empty((n_draw, m), dtype=float)\n",
    "    XoV = Xo.values\n",
    "    XcV = Xc.values\n",
    "\n",
    "    for i in range(n_draw):\n",
    "        b = B[i]\n",
    "        ro = np.exp(XoV @ b)\n",
    "        rc = np.exp(XcV @ b)\n",
    "        sims[i, :] = agg_pct_by_codes(ro, rc, codes, m)\n",
    "\n",
    "    lo = np.nanpercentile(sims, CI_LO, axis=0)\n",
    "    hi = np.nanpercentile(sims, CI_HI, axis=0)\n",
    "\n",
    "    sig = ~((lo <= 0.0) & (hi >= 0.0))\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"iso3\": uniq,\n",
    "        \"Impact\": imp0,\n",
    "        \"CI_lo\": lo,\n",
    "        \"CI_hi\": hi,\n",
    "        \"sig95\": sig.astype(int),\n",
    "    })\n",
    "\n",
    "def impact_ts_by_group_with_ci(beta_hat, cov, Xo, Xc, years, group, group_labels, n_draw=1000, seed=123):\n",
    "    years = years.astype(int).values\n",
    "    g = group.astype(str).values\n",
    "\n",
    "    y_uniq = np.unique(years)\n",
    "    g_uniq = np.array(group_labels, dtype=str)\n",
    "\n",
    "    y_map = {yy:i for i,yy in enumerate(y_uniq)}\n",
    "    g_map = {gg:i for i,gg in enumerate(g_uniq)}\n",
    "\n",
    "    # ËøáÊª§Êéâ group ‰∏çÂú® labels ÈáåÁöÑÔºà‰øùÈô©Ôºâ\n",
    "    keep = np.array([gg in g_map for gg in g], dtype=bool)\n",
    "    years2 = years[keep]\n",
    "    g2 = g[keep]\n",
    "    XoV = Xo.values[keep, :]\n",
    "    XcV = Xc.values[keep, :]\n",
    "\n",
    "    y_code = np.array([y_map[yy] for yy in years2], dtype=int)\n",
    "    g_code = np.array([g_map[gg] for gg in g2], dtype=int)\n",
    "\n",
    "    nY = len(y_uniq)\n",
    "    nG = len(g_uniq)\n",
    "    key = y_code * nG + g_code\n",
    "    nK = nY * nG\n",
    "\n",
    "    ro0 = np.exp(XoV @ beta_hat)\n",
    "    rc0 = np.exp(XcV @ beta_hat)\n",
    "    imp0 = agg_pct_by_codes(ro0, rc0, key, nK).reshape(nY, nG)\n",
    "\n",
    "    B = draw_betas_psd(beta_hat, cov, n=n_draw, seed=seed)\n",
    "    sims = np.empty((n_draw, nY, nG), dtype=float)\n",
    "\n",
    "    for i in range(n_draw):\n",
    "        b = B[i]\n",
    "        ro = np.exp(XoV @ b)\n",
    "        rc = np.exp(XcV @ b)\n",
    "        sims[i, :, :] = agg_pct_by_codes(ro, rc, key, nK).reshape(nY, nG)\n",
    "\n",
    "    lo = np.nanpercentile(sims, CI_LO, axis=0)\n",
    "    hi = np.nanpercentile(sims, CI_HI, axis=0)\n",
    "\n",
    "    out = []\n",
    "    for yi, yy in enumerate(y_uniq):\n",
    "        for gi, gg in enumerate(g_uniq):\n",
    "            out.append((int(yy), str(gg),\n",
    "                        float(imp0[yi, gi]), float(lo[yi, gi]), float(hi[yi, gi])))\n",
    "    df = pd.DataFrame(out, columns=[\"year\",\"SDI_Q\",\"Impact\",\"CI_lo\",\"CI_hi\"])\n",
    "    df[\"sig95\"] = (~((df[\"CI_lo\"] <= 0.0) & (df[\"CI_hi\"] >= 0.0))).astype(int)\n",
    "    return df.sort_values([\"SDI_Q\",\"year\"])\n",
    "\n",
    "# =========================\n",
    "# Plotting\n",
    "# =========================\n",
    "def plot_map_impact_with_hatch(ax, world, df_imp, title):\n",
    "    g = world.merge(df_imp, on=\"iso3\", how=\"left\")\n",
    "\n",
    "    vals = pd.to_numeric(g[\"Impact\"], errors=\"coerce\").values\n",
    "    ok = np.isfinite(vals)\n",
    "    if ok.sum() == 0:\n",
    "        raise ValueError(\"No finite Impact values.\")\n",
    "\n",
    "    v2 = float(np.nanpercentile(vals[ok], 2))\n",
    "    v98 = float(np.nanpercentile(vals[ok], 98))\n",
    "    vmin = max(0.0, v2)\n",
    "    vmax = max(v98, vmin + 1e-6)\n",
    "    norm = Normalize(vmin=vmin, vmax=vmax)\n",
    "\n",
    "    ax.set_title(title, loc=\"left\", fontsize=12)\n",
    "    ax.set_global()\n",
    "    ax.set_facecolor(\"white\")\n",
    "\n",
    "    g.plot(\n",
    "        column=\"Impact\",\n",
    "        ax=ax,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        cmap=\"Reds\",\n",
    "        norm=norm,\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=0.35,\n",
    "        missing_kwds=dict(color=\"#f2f2f2\", edgecolor=\"white\", linewidth=0.25, hatch=\"..\"),\n",
    "        zorder=2,\n",
    "    )\n",
    "\n",
    "    ax.set_extent([-180, 180, -58, 85], crs=ccrs.PlateCarree())\n",
    "    return norm\n",
    "\n",
    "def plot_impact_lines_ci(ax, df_ts, title, sdi_labels, sdi_hex, ylabel):\n",
    "    ax.set_title(title, loc=\"left\", fontsize=12)\n",
    "    if df_ts is None or len(df_ts) == 0:\n",
    "        ax.text(0.5, 0.5, \"No data\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "        return\n",
    "\n",
    "    d = df_ts.copy()\n",
    "    d[\"SDI_Q\"] = d[\"SDI_Q\"].astype(str)\n",
    "\n",
    "    for i, lab in enumerate(sdi_labels):\n",
    "        s = d[d[\"SDI_Q\"] == lab].sort_values(\"year\")\n",
    "        if s.empty:\n",
    "            continue\n",
    "        c = sdi_hex[i] if sdi_hex else None\n",
    "\n",
    "        ax.plot(s[\"year\"], s[\"Impact\"], lw=2.6, color=c, label=lab)\n",
    "        ax.fill_between(s[\"year\"].values, s[\"CI_lo\"].values, s[\"CI_hi\"].values, alpha=0.18, color=c)\n",
    "\n",
    "        ss = s.iloc[-1]\n",
    "        ax.text(int(ss[\"year\"]) + 0.2, float(ss[\"Impact\"]),\n",
    "                f\"{float(ss['Impact']):.2f}%\",\n",
    "                fontsize=9, va=\"center\", color=c)\n",
    "\n",
    "    ax.set_xlabel(\"Year\")\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.legend(frameon=False, ncol=len(sdi_labels), fontsize=9)\n",
    "\n",
    "def impact_unit():\n",
    "    return \"Attributable impact (%)\"\n",
    "\n",
    "# =========================\n",
    "# Data prep\n",
    "# =========================\n",
    "def prep_means_dataset(df_mean, sdi_map_model, labels_model, sdi_map_plot5, labels_plot5, cfg, resid_params):\n",
    "    d = df_mean.copy()\n",
    "\n",
    "    # SDI for model (Q4/Q5)\n",
    "    d[\"SDI_Q\"] = d[\"iso3\"].map(sdi_map_model)\n",
    "    d = d.dropna(subset=[\"SDI_Q\"]).copy()\n",
    "    d[\"SDI_Q\"] = pd.Categorical(d[\"SDI_Q\"], categories=labels_model, ordered=True)\n",
    "\n",
    "    # SDI for plotting (Q5)\n",
    "    d[\"SDI_Q5\"] = d[\"iso3\"].map(sdi_map_plot5)\n",
    "    d[\"SDI_Q5\"] = pd.Categorical(d[\"SDI_Q5\"], categories=labels_plot5, ordered=True)\n",
    "\n",
    "    d[\"HAP_X\"]  = transform_hap_from_raw(d[\"hap_mean\"], cfg[\"hap\"])\n",
    "    d[\"PM25_X\"] = transform_pm25_from_raw(d[\"pm25_mean\"], cfg[\"pm25\"])\n",
    "    d = apply_climate_struct(d, resid_params, is_means=True)\n",
    "    d[\"DENS_X\"] = safe_log1p(pd.to_numeric(d[\"dens_mean\"], errors=\"coerce\"))\n",
    "\n",
    "    if cfg[\"center_inter\"] == \"centerWithinQ\":\n",
    "        d = add_center_within_q(d, cfg[\"rhs\"], group_col=\"SDI_Q\")\n",
    "\n",
    "    need = [\"log_lui_rate_mean\",\"SDI_Q\",\"HAP_X\",\"PM25_X\",\"CLIM1_X\",\"CLIM2_X\",\"DENS_X\"]\n",
    "    if cfg[\"center_inter\"] == \"centerWithinQ\":\n",
    "        for x in [\"HAP_X\",\"PM25_X\",\"CLIM1_X\",\"CLIM2_X\"]:\n",
    "            if f\"{x}_CQ\" in cfg[\"rhs\"]:\n",
    "                need.append(f\"{x}_CQ\")\n",
    "\n",
    "    d = d.dropna(subset=need).copy()\n",
    "    return d\n",
    "\n",
    "def prep_panel_dataset(df_panel, sdi_map_model, labels_model, sdi_map_plot5, labels_plot5, cfg, resid_params):\n",
    "    d = df_panel.copy()\n",
    "\n",
    "    d[\"SDI_Q\"] = d[\"iso3\"].map(sdi_map_model)\n",
    "    d = d.dropna(subset=[\"SDI_Q\"]).copy()\n",
    "    d[\"SDI_Q\"] = pd.Categorical(d[\"SDI_Q\"], categories=labels_model, ordered=True)\n",
    "\n",
    "    d[\"SDI_Q5\"] = d[\"iso3\"].map(sdi_map_plot5)\n",
    "    d[\"SDI_Q5\"] = pd.Categorical(d[\"SDI_Q5\"], categories=labels_plot5, ordered=True)\n",
    "\n",
    "    d[\"HAP_raw\"]  = pd.to_numeric(d[HAP_RAW], errors=\"coerce\")\n",
    "    d[\"PM25_raw\"] = pd.to_numeric(d[PM25_RAW], errors=\"coerce\")\n",
    "    d[\"TAVG\"]     = pd.to_numeric(d[TAVG_RAW], errors=\"coerce\")\n",
    "    d[\"AH\"]       = pd.to_numeric(d[AH_RAW], errors=\"coerce\")\n",
    "    d[\"DENS\"]     = pd.to_numeric(d[DENS_RAW], errors=\"coerce\")\n",
    "\n",
    "    d[\"HAP_X\"]  = transform_hap_from_raw(d[\"HAP_raw\"], cfg[\"hap\"])\n",
    "    d[\"PM25_X\"] = transform_pm25_from_raw(d[\"PM25_raw\"], cfg[\"pm25\"])\n",
    "\n",
    "    d = apply_climate_struct(d, resid_params, is_means=False)\n",
    "    d[\"DENS_X\"] = safe_log1p(d[\"DENS\"])\n",
    "\n",
    "    if cfg[\"center_inter\"] == \"centerWithinQ\":\n",
    "        d = add_center_within_q(d, cfg[\"rhs\"], group_col=\"SDI_Q\")\n",
    "\n",
    "    need = [\"iso3\",\"year\",\"SDI_Q\",\"SDI_Q5\",\"HAP_raw\",\"PM25_raw\",\"HAP_X\",\"PM25_X\",\n",
    "            \"CLIM1_X\",\"CLIM2_X\",\"DENS_X\", cfg[\"pop\"], cfg[\"y\"]]\n",
    "    if cfg[\"center_inter\"] == \"centerWithinQ\":\n",
    "        for x in [\"HAP_X\",\"PM25_X\",\"CLIM1_X\",\"CLIM2_X\"]:\n",
    "            if f\"{x}_CQ\" in cfg[\"rhs\"]:\n",
    "                need.append(f\"{x}_CQ\")\n",
    "\n",
    "    d = d.dropna(subset=need).copy()\n",
    "    d[\"year\"] = pd.to_numeric(d[\"year\"], errors=\"coerce\").astype(int)\n",
    "    d = d[(d[\"year\"] >= Y0) & (d[\"year\"] <= Y1)].copy()\n",
    "    return d\n",
    "\n",
    "# =========================\n",
    "# runner\n",
    "# =========================\n",
    "def run_one_age(df0_age, world, age_key, cfg, tmrel_hap, tmrel_pm25):\n",
    "    out_dir = os.path.join(OUT_ROOT, cfg[\"title_tag\"])\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # means + SDI maps\n",
    "    df_mean = build_iso3_means(df0_age, cfg[\"y\"], cfg[\"pop\"])\n",
    "    sdi_map_model, labels_model, sdi_map_plot5, labels_plot5 = make_sdi_maps_from_means(\n",
    "        df_mean, scheme_model=cfg[\"sdi_scheme\"]\n",
    "    )\n",
    "    resid_params = fit_resid_params_on_means(df_mean, cfg[\"clim_struct\"])\n",
    "\n",
    "    # fit on means\n",
    "    dfit = prep_means_dataset(df_mean, sdi_map_model, labels_model, sdi_map_plot5, labels_plot5, cfg, resid_params)\n",
    "\n",
    "    formula = \"log_lui_rate_mean ~ \" + cfg[\"rhs\"]\n",
    "    res = smf.ols(formula, data=dfit).fit(cov_type=\"HC1\")\n",
    "\n",
    "    beta_hat = res.params.values\n",
    "    cov_hat  = res.cov_params().values\n",
    "\n",
    "    # save summary\n",
    "    with open(os.path.join(out_dir, \"BEST_model_summary_used.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"AGE = \" + age_key + \"\\n\")\n",
    "        f.write(f\"TMREL_HAP={tmrel_hap} | TMREL_PM25={tmrel_pm25}\\n\")\n",
    "        f.write(\"FORMULA:\\n\" + formula + \"\\n\\n\")\n",
    "        f.write(res.summary().as_text())\n",
    "\n",
    "    coef = pd.DataFrame({\n",
    "        \"term\": res.params.index,\n",
    "        \"beta\": res.params.values,\n",
    "        \"se_HC1\": res.bse.values,\n",
    "        \"p_HC1\": res.pvalues.values\n",
    "    }).sort_values(\"p_HC1\")\n",
    "    coef.to_csv(os.path.join(out_dir, \"BEST_model_coef_used.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # panel prep\n",
    "    dpanel = prep_panel_dataset(df0_age, sdi_map_model, labels_model, sdi_map_plot5, labels_plot5, cfg, resid_params)\n",
    "    obs = dpanel.copy()\n",
    "\n",
    "    # ---- build obs design ----\n",
    "    Xo, di = build_design(obs, cfg[\"rhs\"], None)\n",
    "\n",
    "    # ---- counterfactual HAP ----\n",
    "    cf_h = obs.copy()\n",
    "    cf_h_raw = np.minimum(cf_h[\"HAP_raw\"].values.astype(float), float(tmrel_hap))\n",
    "    cf_h[\"HAP_X\"] = transform_hap_from_raw(cf_h_raw, cfg[\"hap\"])\n",
    "    if cfg[\"center_inter\"] == \"centerWithinQ\":\n",
    "        cf_h = add_center_within_q(cf_h, cfg[\"rhs\"], group_col=\"SDI_Q\")\n",
    "    Xh, _ = build_design(cf_h, cfg[\"rhs\"], di)\n",
    "\n",
    "    # ---- counterfactual PM25 ----\n",
    "    cf_p = obs.copy()\n",
    "    cf_p_raw = np.minimum(cf_p[\"PM25_raw\"].values.astype(float), float(tmrel_pm25))\n",
    "    cf_p[\"PM25_X\"] = transform_pm25_from_raw(cf_p_raw, cfg[\"pm25\"])\n",
    "    if cfg[\"center_inter\"] == \"centerWithinQ\":\n",
    "        cf_p = add_center_within_q(cf_p, cfg[\"rhs\"], group_col=\"SDI_Q\")\n",
    "    Xp, _ = build_design(cf_p, cfg[\"rhs\"], di)\n",
    "\n",
    "    # =========================\n",
    "    # MAP 2019: Impact + CI + sig\n",
    "    # =========================\n",
    "    d2019 = obs[obs[\"year\"] == MAP_YEAR].copy()\n",
    "    if d2019.empty:\n",
    "        print(f\"[SKIP MAP] {age_key} no {MAP_YEAR} rows.\")\n",
    "        return\n",
    "\n",
    "    d2019_o = d2019.copy()\n",
    "    d2019_h = d2019.copy()\n",
    "    d2019_p = d2019.copy()\n",
    "\n",
    "    d2019_h_raw = np.minimum(d2019_h[\"HAP_raw\"].values.astype(float), float(tmrel_hap))\n",
    "    d2019_h[\"HAP_X\"] = transform_hap_from_raw(d2019_h_raw, cfg[\"hap\"])\n",
    "\n",
    "    d2019_p_raw = np.minimum(d2019_p[\"PM25_raw\"].values.astype(float), float(tmrel_pm25))\n",
    "    d2019_p[\"PM25_X\"] = transform_pm25_from_raw(d2019_p_raw, cfg[\"pm25\"])\n",
    "\n",
    "    if cfg[\"center_inter\"] == \"centerWithinQ\":\n",
    "        d2019_o = add_center_within_q(d2019_o, cfg[\"rhs\"], group_col=\"SDI_Q\")\n",
    "        d2019_h = add_center_within_q(d2019_h, cfg[\"rhs\"], group_col=\"SDI_Q\")\n",
    "        d2019_p = add_center_within_q(d2019_p, cfg[\"rhs\"], group_col=\"SDI_Q\")\n",
    "\n",
    "    X2019_o, _ = build_design(d2019_o, cfg[\"rhs\"], di)\n",
    "    X2019_h, _ = build_design(d2019_h, cfg[\"rhs\"], di)\n",
    "    X2019_p, _ = build_design(d2019_p, cfg[\"rhs\"], di)\n",
    "\n",
    "    imp_hap_map_ci = impact_map_with_ci(beta_hat, cov_hat, X2019_o, X2019_h, d2019_o[\"iso3\"],\n",
    "                                        n_draw=SIM_N, seed=SIM_SEED)\n",
    "    imp_pm_map_ci  = impact_map_with_ci(beta_hat, cov_hat, X2019_o, X2019_p, d2019_o[\"iso3\"],\n",
    "                                        n_draw=SIM_N, seed=SIM_SEED + 1)\n",
    "\n",
    "    imp_hap_map_ci.to_csv(os.path.join(out_dir, f\"IMPACTMAP_HAP_{MAP_YEAR}_pct_{age_key}_CI.csv\"),\n",
    "                          index=False, encoding=\"utf-8-sig\")\n",
    "    imp_pm_map_ci.to_csv(os.path.join(out_dir, f\"IMPACTMAP_PM25_{MAP_YEAR}_pct_{age_key}_CI.csv\"),\n",
    "                         index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # =========================\n",
    "    # TS: Êåâ Q5 Â±ïÁ§∫Ôºà‰Ω†ÊÉ≥Ë¶ÅÁöÑÔºâ\n",
    "    # =========================\n",
    "    ts_hap_ci = impact_ts_by_group_with_ci(beta_hat, cov_hat, Xo, Xh,\n",
    "                                          years=obs[\"year\"], group=obs[\"SDI_Q5\"],\n",
    "                                          group_labels=SDI_LABELS_Q5,\n",
    "                                          n_draw=SIM_N, seed=SIM_SEED)\n",
    "    ts_pm_ci  = impact_ts_by_group_with_ci(beta_hat, cov_hat, Xo, Xp,\n",
    "                                          years=obs[\"year\"], group=obs[\"SDI_Q5\"],\n",
    "                                          group_labels=SDI_LABELS_Q5,\n",
    "                                          n_draw=SIM_N, seed=SIM_SEED + 1)\n",
    "\n",
    "    ts_hap_ci.to_csv(os.path.join(out_dir, f\"IMPACTTS_HAP_pct_byQ5_{age_key}_CI.csv\"),\n",
    "                     index=False, encoding=\"utf-8-sig\")\n",
    "    ts_pm_ci.to_csv(os.path.join(out_dir, f\"IMPACTTS_PM25_pct_byQ5_{age_key}_CI.csv\"),\n",
    "                    index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # =========================\n",
    "    # Plot 2x2\n",
    "    # =========================\n",
    "    fig = plt.figure(figsize=(13.8, 7.6))\n",
    "    gs = fig.add_gridspec(2, 2, wspace=0.06, hspace=0.22)\n",
    "\n",
    "    ax1 = fig.add_subplot(gs[0,0], projection=ccrs.Robinson())\n",
    "    ax2 = fig.add_subplot(gs[0,1], projection=ccrs.Robinson())\n",
    "    ax3 = fig.add_subplot(gs[1,0])\n",
    "    ax4 = fig.add_subplot(gs[1,1])\n",
    "\n",
    "    unit_map = impact_unit()\n",
    "    unit_ts  = impact_unit()\n",
    "\n",
    "    n1 = plot_map_impact_with_hatch(\n",
    "        ax1, world, imp_hap_map_ci,\n",
    "        f\"a | {age_key} impact (%) due to HAP ({MAP_YEAR})  (//: n.s.)\"\n",
    "    )\n",
    "    n2 = plot_map_impact_with_hatch(\n",
    "        ax2, world, imp_pm_map_ci,\n",
    "        f\"b | {age_key} impact (%) due to PM2.5 ({MAP_YEAR})  (//: n.s.)\"\n",
    "    )\n",
    "\n",
    "    # colorbars\n",
    "    smap1 = mpl.cm.ScalarMappable(norm=n1, cmap=\"Reds\"); smap1.set_array([])\n",
    "    cax1 = fig.add_axes([0.47, 0.56, 0.015, 0.32])\n",
    "    cb1 = plt.colorbar(smap1, cax=cax1); cb1.set_label(unit_map)\n",
    "\n",
    "    smap2 = mpl.cm.ScalarMappable(norm=n2, cmap=\"Reds\"); smap2.set_array([])\n",
    "    cax2 = fig.add_axes([0.92, 0.56, 0.015, 0.32])\n",
    "    cb2 = plt.colorbar(smap2, cax=cax2); cb2.set_label(unit_map)\n",
    "\n",
    "    plot_impact_lines_ci(\n",
    "        ax3, ts_hap_ci,\n",
    "        f\"c | {age_key} impact(t) (%) due to HAP by SDI (plot=Q5; model={cfg['sdi_scheme']})\",\n",
    "        SDI_LABELS_Q5, SDI_HEX_Q5, ylabel=unit_ts\n",
    "    )\n",
    "    plot_impact_lines_ci(\n",
    "        ax4, ts_pm_ci,\n",
    "        f\"d | {age_key} impact(t) (%) due to PM2.5 by SDI (plot=Q5; model={cfg['sdi_scheme']})\",\n",
    "        SDI_LABELS_Q5, SDI_HEX_Q5, ylabel=unit_ts\n",
    "    )\n",
    "\n",
    "    out_png = os.path.join(out_dir, f\"FIG_air_2x2_IMPACTpct_{age_key}_Q5plot_CI_HATCH.png\")\n",
    "    out_pdf = os.path.join(out_dir, f\"FIG_air_2x2_IMPACTpct_{age_key}_Q5plot_CI_HATCH.pdf\")\n",
    "    plt.savefig(out_png, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.savefig(out_pdf, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "    # ---- debug: 2019 ÊúâÂ§öÂ∞ëÂõΩÂÆ∂ PM25_raw <= TMRELÔºàÂØºËá¥ cf‚âàobsÔºåimpact‚âà0Ôºâ----\n",
    "    d2019_debug = dpanel[dpanel[\"year\"] == MAP_YEAR].copy()\n",
    "    if not d2019_debug.empty:\n",
    "        iso_m = d2019_debug.groupby(\"iso3\")[\"PM25_raw\"].mean()\n",
    "        share_small = float((iso_m <= tmrel_pm25).mean())\n",
    "        print(f\"[DEBUG] {age_key} 2019 share( mean PM25_raw <= TMREL_PM25 ) = {share_small:.3f}\")\n",
    "\n",
    "    print(f\"[OK] age={age_key} -> {out_dir}\")\n",
    "\n",
    "# =========================\n",
    "# MAIN\n",
    "# =========================\n",
    "def main():\n",
    "    # TMREL ‰∏éÂèòÈáèÂ∞∫Â∫¶‰∏ÄËá¥\n",
    "    if AUTO_SCALE_DIV10:\n",
    "        tmrel_hap  = TMREL_HAP_BASE / 10.0\n",
    "        tmrel_pm25 = TMREL_PM25_BASE / 10.0\n",
    "    else:\n",
    "        tmrel_hap  = TMREL_HAP_BASE\n",
    "        tmrel_pm25 = TMREL_PM25_BASE\n",
    "\n",
    "    print(f\"[TMREL] AUTO_SCALE_DIV10={AUTO_SCALE_DIV10} | TMREL_HAP={tmrel_hap} | TMREL_PM25={tmrel_pm25}\")\n",
    "\n",
    "    df0 = pd.read_csv(IN_FP)\n",
    "    df0[\"iso3\"] = df0[\"iso3\"].astype(str).str.upper().str.strip()\n",
    "    df0[\"year\"] = pd.to_numeric(df0[\"year\"], errors=\"coerce\")\n",
    "\n",
    "    need = [\"iso3\",\"year\",HAP_RAW,PM25_RAW,TAVG_RAW,AH_RAW,DENS_RAW,SDI_RAW]\n",
    "    for _, cfg in AGE_CFG.items():\n",
    "        need += [cfg[\"y\"], cfg[\"pop\"]]\n",
    "    need = sorted(set(need))\n",
    "    require_cols(df0, need)\n",
    "\n",
    "    for c in need:\n",
    "        if c != \"iso3\":\n",
    "            df0[c] = pd.to_numeric(df0[c], errors=\"coerce\")\n",
    "\n",
    "    df0 = df0.dropna(subset=[\"iso3\",\"year\"]).copy()\n",
    "    df0 = df0[(df0[\"year\"]>=Y0) & (df0[\"year\"]<=Y1)].copy()\n",
    "    df0[\"year\"] = df0[\"year\"].astype(int)\n",
    "\n",
    "    # Âø´ÈÄüÊ£ÄÊü•ÂèòÈáèÂ∞∫Â∫¶ÔºàÂà§Êñ≠ /10 ÊòØÂê¶ÂêàÁêÜÔºâ\n",
    "    print(\"[CHECK] PM25_raw median/p95:\",\n",
    "          float(df0[PM25_RAW].median()), float(df0[PM25_RAW].quantile(0.95)))\n",
    "    print(\"[CHECK] HAP_raw  median/p95:\",\n",
    "          float(df0[HAP_RAW].median()), float(df0[HAP_RAW].quantile(0.95)))\n",
    "\n",
    "    world = load_world(SHP_FP)\n",
    "\n",
    "    for age_key, cfg in AGE_CFG.items():\n",
    "        cols_age = [cfg[\"y\"], cfg[\"pop\"], HAP_RAW, PM25_RAW, TAVG_RAW, AH_RAW, DENS_RAW, SDI_RAW]\n",
    "        d = df0.dropna(subset=cols_age).copy()\n",
    "        if d.empty:\n",
    "            print(f\"[SKIP] age={age_key} empty after dropna.\")\n",
    "            continue\n",
    "\n",
    "        print(\"\\n\" + \"=\"*96)\n",
    "        print(f\"RUN age={age_key} | model SDI scheme={cfg['sdi_scheme']} | plot=Q5 | TMREL={tmrel_pm25}\")\n",
    "        print(\"=\"*96)\n",
    "\n",
    "        run_one_age(d, world, age_key, cfg, tmrel_hap=tmrel_hap, tmrel_pm25=tmrel_pm25)\n",
    "\n",
    "    print(\"\\nDONE. OUT_ROOT =\", OUT_ROOT)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4659d6cb-17cf-4835-a314-59ef240a270f",
   "metadata": {},
   "source": [
    "## Robustness check using two-way fixed effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51de3b12-0b51-4955-9fa1-b37affc38e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import geopandas as gpd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# STYLE\n",
    "# =========================\n",
    "mpl.rcParams[\"pdf.fonttype\"] = 42\n",
    "mpl.rcParams[\"ps.fonttype\"]  = 42\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"Arial\", \"SimHei\", \"DejaVu Sans\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "# =========================\n",
    "# PATHS\n",
    "# =========================\n",
    "IN_FP  = r\"D:\\AAUDE\\paper_v2\\paper2\\data\\model_outputs\\panel0_1990_2019_direct_meteo_GBDPM_HAP_lui.csv\"\n",
    "SHP_FP = r\"D:\\AAUDE\\paper_v2\\paper2\\data\\ne_10m_admin_0_countries\\ne_10m_admin_0_countries.shp\"\n",
    "OUT_DIR = r\"D:\\AAUDE\\paper_v2\\paper2\\data\\model_outputs\\AF_PAF_space_time_v1\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# CONFIG: choose age group\n",
    "# =========================\n",
    "# ‰Ω†ËøôÈáå uri_total ÂÆûÈôÖÊòØ LUI_totalÔºàÂêçÂ≠óÊ≤°ÊîπÔºâ\n",
    "Y_COL   = \"uri_total\"\n",
    "POP_COL = \"pop_total\"\n",
    "\n",
    "# exposures (raw)\n",
    "HAP_COL  = \"hap_pm_pw\"\n",
    "PM25_COL = \"pm25_pw\"\n",
    "\n",
    "# controls (raw)\n",
    "TAVG_COL = \"tavg_pw_C\"\n",
    "AH_COL   = \"ah_pw\"\n",
    "DENS_COL = \"density_total_pkm2\"\n",
    "SDI_COL  = \"sdi\"\n",
    "\n",
    "# years\n",
    "Y0, Y1 = 1990, 2019\n",
    "MAP_YEAR = 2019\n",
    "\n",
    "# TMREL (counterfactual)\n",
    "TMREL_HAP  = 5.0\n",
    "TMREL_PM25 = 5.0\n",
    "\n",
    "# model family\n",
    "USE_NB = False        # True=NB(alpha fixed), False=Poisson\n",
    "ALPHA_NB = 1.0        # only used if USE_NB=True\n",
    "\n",
    "# SDI grouping (stable by country mean SDI)\n",
    "SDI_Q = 5\n",
    "SDI_HEX = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\", \"#9467bd\"]  # Q1..Q5\n",
    "# =========================\n",
    "# helpers\n",
    "# =========================\n",
    "def require_cols(df, cols):\n",
    "    miss = [c for c in cols if c not in df.columns]\n",
    "    if miss:\n",
    "        raise ValueError(f\"Missing columns: {miss}\")\n",
    "\n",
    "def load_world_shp(shp_fp):\n",
    "    g = gpd.read_file(shp_fp)\n",
    "    cand = [\"ADM0_A3\", \"ISO_A3\", \"SOV_A3\", \"WB_A3\", \"ISO3\", \"iso3\"]\n",
    "    key = None\n",
    "    for c in cand:\n",
    "        if c in g.columns:\n",
    "            key = c\n",
    "            break\n",
    "    if key is None:\n",
    "        raise ValueError(f\"Shapefile lacks iso3 field. Tried: {cand}. Available: {list(g.columns)}\")\n",
    "\n",
    "    g = g.rename(columns={key: \"iso3\"})\n",
    "    g[\"iso3\"] = g[\"iso3\"].astype(str).str.upper().str.strip()\n",
    "    g.loc[g[\"iso3\"] == \"-99\", \"iso3\"] = np.nan\n",
    "    g = g[~g[\"iso3\"].isna()].copy()\n",
    "\n",
    "    # ‚úÖ drop Antarctica\n",
    "    g = g[g[\"iso3\"] != \"ATA\"].copy()\n",
    "\n",
    "    return g\n",
    "def plot_paf_map(world_gdf, paf_df, title, out_fp):\n",
    "    g = world_gdf.merge(paf_df, on=\"iso3\", how=\"left\").copy()\n",
    "\n",
    "    vals = pd.to_numeric(g[\"PAF\"], errors=\"coerce\").to_numpy()\n",
    "    finite = np.isfinite(vals)\n",
    "    if finite.sum() == 0:\n",
    "        raise ValueError(\"No finite PAF values to plot.\")\n",
    "\n",
    "    # robust symmetric limits\n",
    "    v2 = np.nanpercentile(vals[finite], 2)\n",
    "    v98 = np.nanpercentile(vals[finite], 98)\n",
    "    lim = float(max(abs(v2), abs(v98)))\n",
    "    vmin, vmax = -lim, lim\n",
    "    norm = TwoSlopeNorm(vmin=vmin, vcenter=0.0, vmax=vmax)\n",
    "\n",
    "    fig = plt.figure(figsize=(12.2, 6.0))\n",
    "    ax = plt.axes(projection=ccrs.Robinson())\n",
    "    ax.set_global()\n",
    "\n",
    "    # ‚úÖ no ocean background\n",
    "    ax.set_facecolor(\"white\")\n",
    "\n",
    "    # ‚úÖ land base only (no ocean feature)\n",
    "    ax.add_feature(cfeature.LAND, facecolor=\"#d9d9d9\", zorder=0)\n",
    "    ax.coastlines(linewidth=0.25, alpha=0.6)\n",
    "\n",
    "    # plot countries\n",
    "    g.plot(\n",
    "        column=\"PAF\",\n",
    "        ax=ax,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        cmap=\"RdBu_r\",\n",
    "        norm=norm,\n",
    "        edgecolor=\"#4c4c4c\",\n",
    "        linewidth=0.25,\n",
    "        missing_kwds=dict(\n",
    "            color=\"#f2f2f2\", edgecolor=\"#4c4c4c\", linewidth=0.2, hatch=\"..\", label=\"Missing\"\n",
    "        ),\n",
    "        zorder=2,\n",
    "    )\n",
    "\n",
    "    ax.set_title(title, fontsize=13)\n",
    "\n",
    "    # colorbar\n",
    "    smap = mpl.cm.ScalarMappable(norm=norm, cmap=\"RdBu_r\")\n",
    "    smap.set_array([])\n",
    "    cbar = plt.colorbar(smap, ax=ax, fraction=0.03, pad=0.03)\n",
    "    cbar.set_label(\"PAF (fraction)\", fontsize=11)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_fp, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    print(\"Saved:\", out_fp)\n",
    "def af_timeseries_by_sdiq_ci(res, df, x_col, x0, n_boot=300, seed=123):\n",
    "    \"\"\"\n",
    "    Bootstrap CI for AF(t) by SDI quintiles.\n",
    "    Resample iso3 with replacement; within each bootstrap, compute AF for each year√óSDI_Q.\n",
    "    Returns: df with columns [year, SDI_Q, AF, ci_lo, ci_hi]\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    iso_list = np.array(sorted(df[\"iso3\"].unique()))\n",
    "    years = np.array(sorted(df[\"year\"].unique()))\n",
    "    qs = np.arange(1, SDI_Q + 1)\n",
    "\n",
    "    # point estimate\n",
    "    df_point = af_timeseries_by_sdiq(res, df, x_col=x_col, x0=x0)\n",
    "    df_point = df_point[[\"year\",\"SDI_Q\",\"AF\"]].copy()\n",
    "\n",
    "    # store boot AF\n",
    "    boot_rec = []\n",
    "\n",
    "    for b in range(n_boot):\n",
    "        boot_iso = rng.choice(iso_list, size=len(iso_list), replace=True)\n",
    "        # build boot sample by concatenating country blocks\n",
    "        dboot = pd.concat([df[df[\"iso3\"] == iso].copy() for iso in boot_iso], ignore_index=True)\n",
    "\n",
    "        # compute AF\n",
    "        dab = af_timeseries_by_sdiq(res, dboot, x_col=x_col, x0=x0)\n",
    "        dab = dab[[\"year\",\"SDI_Q\",\"AF\"]].copy()\n",
    "        dab[\"b\"] = b\n",
    "        boot_rec.append(dab)\n",
    "\n",
    "    dfb = pd.concat(boot_rec, ignore_index=True)\n",
    "\n",
    "    # CI\n",
    "    qlo, qhi = 0.025, 0.975\n",
    "    ci = (\n",
    "        dfb.groupby([\"year\",\"SDI_Q\"])[\"AF\"]\n",
    "           .quantile([qlo, qhi])\n",
    "           .unstack(level=-1)\n",
    "           .reset_index()\n",
    "           .rename(columns={qlo:\"ci_lo\", qhi:\"ci_hi\"})\n",
    "    )\n",
    "\n",
    "    out = df_point.merge(ci, on=[\"year\",\"SDI_Q\"], how=\"left\")\n",
    "    return out\n",
    "\n",
    "def prepare_panel(df_raw):\n",
    "    df = df_raw.copy()\n",
    "    df[\"iso3\"] = df[\"iso3\"].astype(str).str.upper().str.strip()\n",
    "    df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\")\n",
    "\n",
    "    need = [\"iso3\", \"year\", Y_COL, POP_COL, HAP_COL, PM25_COL, TAVG_COL, AH_COL, DENS_COL, SDI_COL]\n",
    "    require_cols(df, need)\n",
    "\n",
    "    # numeric\n",
    "    for c in [Y_COL, POP_COL, HAP_COL, PM25_COL, TAVG_COL, AH_COL, DENS_COL, SDI_COL]:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    df = df.dropna(subset=need).copy()\n",
    "    df = df[(df[\"year\"] >= Y0) & (df[\"year\"] <= Y1)].copy()\n",
    "    df[\"year\"] = df[\"year\"].astype(int)\n",
    "\n",
    "    # offset & transforms\n",
    "    df[\"offset_log\"] = np.log(df[POP_COL].clip(lower=1.0))\n",
    "    df[\"log1p_density\"] = np.log1p(df[DENS_COL].clip(lower=0.0))\n",
    "\n",
    "    # stable SDI quintile by country mean SDI\n",
    "    df[\"sdi_bar\"] = df.groupby(\"iso3\")[SDI_COL].transform(\"mean\")\n",
    "    sdi_country = df.groupby(\"iso3\")[\"sdi_bar\"].first()\n",
    "    grp = pd.qcut(sdi_country, q=SDI_Q, labels=False, duplicates=\"drop\") + 1\n",
    "    df[\"SDI_Q\"] = df[\"iso3\"].map(grp).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "def fit_country_fe_model(df):\n",
    "    formula = (\n",
    "        f\"{Y_COL} ~ year + C(iso3)\"\n",
    "        f\" + {HAP_COL} + {PM25_COL}\"\n",
    "        f\" + {TAVG_COL} + {AH_COL}\"\n",
    "        f\" + log1p_density\"\n",
    "    )\n",
    "    fam = sm.families.NegativeBinomial(alpha=ALPHA_NB) if USE_NB else sm.families.Poisson()\n",
    "    res = smf.glm(\n",
    "        formula=formula,\n",
    "        data=df,\n",
    "        family=fam,\n",
    "        offset=df[\"offset_log\"],\n",
    "    ).fit(maxiter=200, disp=0)\n",
    "    return res, formula\n",
    "\n",
    "def predict_mu(res, d):\n",
    "    return np.asarray(res.predict(d, offset=d[\"offset_log\"]), dtype=float)\n",
    "\n",
    "# ---- PAF for one year, one exposure\n",
    "def paf_country_year(res, d_year, x_col, x0):\n",
    "    out = []\n",
    "    for iso, di in d_year.groupby(\"iso3\", sort=False):\n",
    "        di = di.copy()\n",
    "        mu_obs = predict_mu(res, di)\n",
    "\n",
    "        di_cf = di.copy()\n",
    "        di_cf[x_col] = float(x0)\n",
    "        mu_cf = predict_mu(res, di_cf)\n",
    "\n",
    "        den = float(mu_obs.sum())\n",
    "        paf = np.nan if den <= 0 else float((mu_obs.sum() - mu_cf.sum()) / den)\n",
    "        out.append({\"iso3\": iso, \"PAF\": paf, \"x0\": float(x0)})\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "# ---- AF time series by SDI quintile for one exposure\n",
    "def af_timeseries_by_sdiq(res, df, x_col, x0):\n",
    "    rec = []\n",
    "    for y in sorted(df[\"year\"].unique()):\n",
    "        dy = df[df[\"year\"] == y].copy()\n",
    "        if dy.empty:\n",
    "            continue\n",
    "        mu_obs = predict_mu(res, dy)\n",
    "\n",
    "        dcf = dy.copy()\n",
    "        dcf[x_col] = float(x0)\n",
    "        mu_cf = predict_mu(res, dcf)\n",
    "\n",
    "        for q in range(1, SDI_Q + 1):\n",
    "            idx = (dy[\"SDI_Q\"] == q).to_numpy()\n",
    "            if idx.sum() == 0:\n",
    "                continue\n",
    "            sum_obs = float(mu_obs[idx].sum())\n",
    "            delta = float((mu_obs[idx] - mu_cf[idx]).sum())\n",
    "            af = float(delta / max(sum_obs, 1e-12))\n",
    "            rec.append({\n",
    "                \"year\": int(y),\n",
    "                \"SDI_Q\": int(q),\n",
    "                \"AF\": af,\n",
    "                \"DeltaCases\": delta,\n",
    "                \"sum_mu_obs\": sum_obs,\n",
    "                \"x_col\": x_col,\n",
    "                \"x0\": float(x0),\n",
    "                \"n_rows\": int(idx.sum()),\n",
    "            })\n",
    "    return pd.DataFrame(rec)\n",
    "\n",
    "def plot_af_lines(df_af, title, out_png, out_pdf):\n",
    "    plt.figure(figsize=(8.8, 5.0))\n",
    "\n",
    "    for q in range(1, SDI_Q + 1):\n",
    "        s = df_af[df_af[\"SDI_Q\"] == q].sort_values(\"year\")\n",
    "        if s.empty:\n",
    "            continue\n",
    "\n",
    "        plt.plot(s[\"year\"], s[\"AF\"], lw=2.6, color=SDI_HEX[q-1], label=f\"Q{q}\")\n",
    "\n",
    "        # ‚úÖ CI shading if exists\n",
    "        if (\"ci_lo\" in s.columns) and (\"ci_hi\" in s.columns):\n",
    "            if np.isfinite(s[\"ci_lo\"]).any() and np.isfinite(s[\"ci_hi\"]).any():\n",
    "                plt.fill_between(\n",
    "                    s[\"year\"],\n",
    "                    s[\"ci_lo\"],\n",
    "                    s[\"ci_hi\"],\n",
    "                    color=SDI_HEX[q-1],\n",
    "                    alpha=0.12,\n",
    "                    linewidth=0\n",
    "                )\n",
    "\n",
    "    plt.axhline(0, lw=1, color=\"k\", alpha=0.6)\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Attributable fraction (AF)\")\n",
    "    plt.title(title)\n",
    "    plt.legend(frameon=False, ncol=5)\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.savefig(out_pdf, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print(\"Saved:\", out_png)\n",
    "    print(\"Saved:\", out_pdf)\n",
    "\n",
    "# =========================\n",
    "# MAIN\n",
    "# =========================\n",
    "def main():\n",
    "    print(\"Loading:\", IN_FP)\n",
    "    df_raw = pd.read_csv(IN_FP)\n",
    "    df = prepare_panel(df_raw)\n",
    "\n",
    "    print(\"Prepared:\", df.shape, \"| iso3:\", df[\"iso3\"].nunique(), \"| years:\", df[\"year\"].min(), \"-\", df[\"year\"].max())\n",
    "    print(\"SDI_Q counts:\", df[\"SDI_Q\"].value_counts().sort_index().to_dict())\n",
    "\n",
    "    # fit model\n",
    "    res, formula = fit_country_fe_model(df)\n",
    "    print(\"\\n[MODEL]\\n\", formula)\n",
    "    print(\"[AIC]\", float(res.aic))\n",
    "\n",
    "    # save summary\n",
    "    summ_fp = os.path.join(OUT_DIR, \"model_summary.txt\")\n",
    "    with open(summ_fp, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"FORMULA:\\n\" + formula + \"\\n\\n\")\n",
    "        f.write(res.summary().as_text())\n",
    "    print(\"Saved:\", summ_fp)\n",
    "\n",
    "    # ---------- (1) SPATIAL MAPS: PAF in MAP_YEAR ----------\n",
    "    world = load_world_shp(SHP_FP)\n",
    "    d_year = df[df[\"year\"] == MAP_YEAR].copy()\n",
    "    if d_year.empty:\n",
    "        raise ValueError(f\"No rows for MAP_YEAR={MAP_YEAR}\")\n",
    "\n",
    "    # HAP map\n",
    "    paf_hap = paf_country_year(res, d_year, x_col=HAP_COL, x0=TMREL_HAP)\n",
    "    paf_hap_fp = os.path.join(OUT_DIR, f\"PAF_HAP_{MAP_YEAR}_details.csv\")\n",
    "    paf_hap.to_csv(paf_hap_fp, index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"Saved:\", paf_hap_fp)\n",
    "\n",
    "    plot_paf_map(\n",
    "        world, paf_hap,\n",
    "        title=f\"PAF_HAP ({MAP_YEAR}) | CF: {HAP_COL}={TMREL_HAP} | allow +/-\",\n",
    "        out_fp=os.path.join(OUT_DIR, f\"PAF_HAP_{MAP_YEAR}.pdf\")\n",
    "    )\n",
    "\n",
    "    # PM2.5 map\n",
    "    paf_pm = paf_country_year(res, d_year, x_col=PM25_COL, x0=TMREL_PM25)\n",
    "    paf_pm_fp = os.path.join(OUT_DIR, f\"PAF_PM25_{MAP_YEAR}_details.csv\")\n",
    "    paf_pm.to_csv(paf_pm_fp, index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"Saved:\", paf_pm_fp)\n",
    "\n",
    "    plot_paf_map(\n",
    "        world, paf_pm,\n",
    "        title=f\"PAF_PM2.5 ({MAP_YEAR}) | CF: {PM25_COL}={TMREL_PM25} | allow +/-\",\n",
    "        out_fp=os.path.join(OUT_DIR, f\"PAF_PM25_{MAP_YEAR}.pdf\")\n",
    "    )\n",
    "\n",
    "    # ---------- (2) TEMPORAL: AF(t) by SDI quintiles ----------\n",
    "    af_hap = af_timeseries_by_sdiq_ci(res, df, x_col=HAP_COL, x0=TMREL_HAP, n_boot=300, seed=123)\n",
    "    af_hap_fp = os.path.join(OUT_DIR, \"AF_timeseries_bySDIQ_HAP_withCI.csv\")\n",
    "    af_hap.to_csv(af_hap_fp, index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"Saved:\", af_hap_fp)\n",
    "    \n",
    "    plot_af_lines(\n",
    "        af_hap,\n",
    "        title=f\"AF(t) by SDI quintiles | exposure=HAP | CF {HAP_COL}={TMREL_HAP}\",\n",
    "        out_png=os.path.join(OUT_DIR, \"AF_bySDIQ_HAP_withCI.png\"),\n",
    "        out_pdf=os.path.join(OUT_DIR, \"AF_bySDIQ_HAP_withCI.pdf\"),\n",
    "    )\n",
    "    \n",
    "    af_pm = af_timeseries_by_sdiq_ci(res, df, x_col=PM25_COL, x0=TMREL_PM25, n_boot=300, seed=123)\n",
    "    af_pm_fp = os.path.join(OUT_DIR, \"AF_timeseries_bySDIQ_PM25_withCI.csv\")\n",
    "    af_pm.to_csv(af_pm_fp, index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"Saved:\", af_pm_fp)\n",
    "    \n",
    "    plot_af_lines(\n",
    "        af_pm,\n",
    "        title=f\"AF(t) by SDI quintiles | exposure=PM2.5 | CF {PM25_COL}={TMREL_PM25}\",\n",
    "        out_png=os.path.join(OUT_DIR, \"AF_bySDIQ_PM25_withCI.png\"),\n",
    "        out_pdf=os.path.join(OUT_DIR, \"AF_bySDIQ_PM25_withCI.pdf\"),\n",
    "    )\n",
    "\n",
    "\n",
    "    print(\"\\nDONE. OUT_DIR =\", OUT_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
